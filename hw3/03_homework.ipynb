{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1 (4 балла)\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05357a75",
   "metadata": {},
   "source": [
    "## Решение:\n",
    "\n",
    "*NOTE: я не знаю, как умудрился получить такие результаты. Прошу прощения за полотна дублированного кода.*\n",
    "\n",
    "*NOTE2: прогонял заново на нормальном компьютере и со скоростью всё хорошо, но блок с оптимизацией решил оставить*\n",
    "\n",
    "### Загрузка датасета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5814a100-9ba8-4787-b0ac-73fa657a3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "corpus_path = pathlib.Path(r\"data/corpus_wsd_50k.txt\")\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.read().split('\\n\\n')\n",
    "    corpus = map(\n",
    "        lambda sent: [\n",
    "            word.split(\"\\t\")\n",
    "            for word\n",
    "            in sent.split(\"\\n\")\n",
    "            ],\n",
    "        corpus\n",
    "        )\n",
    "    corpus = tuple(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b009fee",
   "metadata": {},
   "source": [
    "###  Загрузка ворднета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbeefc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/smertlove/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/smertlove/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet as wn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f90e0d",
   "metadata": {},
   "source": [
    "###  Функция, составляющая контекст\n",
    "\n",
    "Конетекстом считаем предложение, в котором встречается слово, без самого слова. Будем брать предложение из датасета целиком и маскировать в нем слово, для которого нужно определить значение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc38e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask_target(corpus_sentence:list, target_ind:int) -> list:\n",
    "\n",
    "    result = list()\n",
    "\n",
    "    for ind, word in enumerate(corpus_sentence):\n",
    "        if ind != target_ind:\n",
    "            result.append(word[1])\n",
    "        else:\n",
    "            result.append(\"_\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c98d1",
   "metadata": {},
   "source": [
    "### Препроц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa6d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    stopwords:list[str] = nltk.corpus.stopwords.words(\"english\")\n",
    "    stopwords.extend(punctuation)\n",
    "    stopwords.append(\"_\")\n",
    "    stopwords = set(stopwords)\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text:str) -> list[str]:\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "    @classmethod\n",
    "    def rm_stopwords(cls, tokenized_text:list[str]) -> list[str]:\n",
    "        return [c for c in tokenized_text if c not in cls.stopwords]\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, text: str | list[str]) -> list[str]:\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text = cls.tokenize(text)\n",
    "        \n",
    "        text = [c.lower() for c in text]      \n",
    "        text = cls.rm_stopwords(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808a2a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['permit',\n",
       " 'become',\n",
       " 'giveaway',\n",
       " 'rather',\n",
       " 'one',\n",
       " 'goal',\n",
       " 'improved',\n",
       " 'employee',\n",
       " 'morale',\n",
       " 'consequently',\n",
       " 'increased',\n",
       " 'productivity']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверяем препроц\n",
    "\n",
    "s = corpus[1]\n",
    "s = get_context_mask_target(s, 8)\n",
    "s = Preprocessor.preprocess(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c62ab",
   "metadata": {},
   "source": [
    "###  Алгоритм Леска в общем виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43f96fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lesk:\n",
    "\n",
    "    #  Псевдометрика, переопределяется в классах-наследниках\n",
    "    metric = lambda sent1, sent2: 0\n",
    "\n",
    "    @classmethod\n",
    "    def _get_all_definitions(cls, word:str) -> str:\n",
    "        return [c.definition() for c in wn.synsets(word)]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_definition(cls, word:str, context:list[str]) -> str:\n",
    "\n",
    "        all_results = [\n",
    "            {\n",
    "                \"definition\": definition,\n",
    "                \"score\"     : cls.metric(context, Preprocessor.preprocess(definition))\n",
    "            }\n",
    "            for definition\n",
    "            in cls._get_all_definitions(word)\n",
    "        ]\n",
    "\n",
    "        return max(all_results, key=lambda c: c[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3bc82",
   "metadata": {},
   "source": [
    "### Алгоритм Леска с метрикой Жаккара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd31c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(sent1:list[str], sent2:list[str]) -> float:\n",
    "    sent1 = set(sent1)\n",
    "    sent2 = set(sent2)\n",
    "\n",
    "    return len(sent1 & sent2) / len(sent1 | sent2)\n",
    "\n",
    "\n",
    "class LeskWithJaccard(Lesk):\n",
    "    metric = jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e4e9ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant',\n",
       " 'score': 0.3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my _ with my personnel in it to my dads power plant.\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithJaccard.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eba7f63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a motor vehicle with four wheels; usually propelled by an internal combustion engine',\n",
       " 'score': 0.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW _ from the shop and it's amazing\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithJaccard.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90288f5",
   "metadata": {},
   "source": [
    "### Алгоритм Леска с косинусной близостью на эмбедингах sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2ca660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ce9faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebbb790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(sent1:list[str], sent2:list[str]) -> float:\n",
    "    emb1 = model.encode(\" \".join(sent1)).reshape(1, -1)\n",
    "    emb2 = model.encode(\" \".join(sent2)).reshape(1, -1)\n",
    "\n",
    "    return float(cosine_distances(emb1, emb2)[0][0])\n",
    "\n",
    "class LeskWithCosSim(Lesk):\n",
    "    metric = cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bb29206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'where passengers ride up and down',\n",
       " 'score': 0.8515398502349854}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my _ with my personnel in it to my dads power plant.\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3585c07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a wheeled vehicle adapted to the rails of railroad',\n",
       " 'score': 0.9286145567893982}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW _ from the shop and it's amazing\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828ae0b",
   "metadata": {},
   "source": [
    "### Всё готово для сравнительного тестирования\n",
    "\n",
    "Осталось пройти по всему датасету, предсказать значения для всех многозначных слов обоими способами и сравнить результаты с эталонными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71ced77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "542dc619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                context\n",
    "            )\n",
    "\n",
    "            cos_sim_result = LeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e04ec",
   "metadata": {},
   "source": [
    "Первые 10 предложений в корпусе обрабатываются 4 с лишним минуты. Что вдвойне обидно, если принять во внимание тот факт, что в коде выше я забыл записать результаты замеров. Тем не менее, есть подозрение, что функция расчета косинусного подобия работает слишком медленно. Ниже попробую оптимизировать ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55a7192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "##  Тут поменяем интерфейс так, чтобы не было лишних операций str.join\n",
    "##  и заменим функцию расчета косинусной близости, чтобы модель кодировала контекст только 1 раз.\n",
    "def better_cos_sim(context_sent:str, other_sents:list[str]) -> np.ndarray:\n",
    "    context_emb = model.encode(context_sent).reshape(1, -1)\n",
    "    other_embs = [\n",
    "        model.encode(sent)\n",
    "        for sent\n",
    "        in other_sents\n",
    "    ]\n",
    "    return cosine_distances(context_emb, other_embs)[0]\n",
    "\n",
    "\n",
    "##  Тут поменяем логику под новый расчет метрики\n",
    "class BetterLeskWithCosSim(Lesk):\n",
    "    \n",
    "    metric = better_cos_sim\n",
    "\n",
    "    @classmethod\n",
    "    def get_definition(cls, word:str, context:str) -> str:\n",
    "\n",
    "        definitions = cls._get_all_definitions(word)\n",
    "        metrics: np.ndarray = cls.metric(context, definitions)\n",
    "\n",
    "        argmax = metrics.argmax()\n",
    "\n",
    "        return {\n",
    "            \"definition\": definitions[argmax],\n",
    "            \"score\"     : float(metrics[argmax])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e21c9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant',\n",
       " 'score': 0.7972331047058105}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my _ with my personnel in it to my dads power plant.\"\n",
    "ctx = \" \".join(Preprocessor.preprocess(ctx))\n",
    "\n",
    "BetterLeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f0c77a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a conveyance for passengers or freight on a cable railway',\n",
       " 'score': 0.9488926529884338}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW _ from the shop and it's amazing\"\n",
    "ctx = \" \".join(Preprocessor.preprocess(ctx))\n",
    "\n",
    "BetterLeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b871014",
   "metadata": {},
   "source": [
    "Результаты другие. Возможно, изначально метрика была посчитана неверно. В любом случае пока что кажется, что имеется неплохое ускорение. Ниже дублирую код замеров, не забыв при этом записать результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16d2561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "            context = Preprocessor.preprocess(context)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                context\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1b0d1",
   "metadata": {},
   "source": [
    "Четырехкратное ускорение, это победа. Отпечатаем результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc8231c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    cos_sim_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result, cos_sim_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "        cos_sim_positive_count += cos_sim_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim % : {cos_sim_positive_count * 100 / total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83682eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim % : 14.925373134328359\n"
     ]
    }
   ],
   "source": [
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623986fe",
   "metadata": {},
   "source": [
    "Ускорение получили, а вот нормально работать в сделку уже не входило. При чем если с жаккаром такие результаты еще можно понять, то с косинусным подобием точно что-то пошло не так. Попробуем убрать препроц в расчете метрики косинусного подобия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ed399dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim % : 19.402985074626866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bcf76",
   "metadata": {},
   "source": [
    "Уже лучше, но все еще плохо. Попробуем вместо максимального значения подобия взять минимальное:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7b69a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterLeskWithCosSim_InverseArgmax(Lesk):\n",
    "    \n",
    "    metric = better_cos_sim\n",
    "\n",
    "    @classmethod\n",
    "    def get_definition(cls, word:str, context:str) -> str:\n",
    "\n",
    "        definitions = cls._get_all_definitions(word)\n",
    "        metrics: np.ndarray = cls.metric(context, definitions)\n",
    "        \n",
    "        #  :o)\n",
    "        argmax = metrics.argmin()\n",
    "\n",
    "        return {\n",
    "            \"definition\": definitions[argmax],\n",
    "            \"score\"     : float(metrics[argmax])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ca3c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim % : 43.28358208955224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083653af",
   "metadata": {},
   "source": [
    "Результаты неожиданные. Посмотрим, что посчитается на следующих 20 предложениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7783c3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 54.30107526881721\n",
      "cos_sim_normal % : 24.193548387096776\n",
      "cos_sim_inverse_argmax % : 40.32258064516129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    cos_sim_positive_count = 0\n",
    "    inverse_argmax_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result, cos_sim_result, inverse_argmax_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "        cos_sim_positive_count += cos_sim_result == gold\n",
    "        inverse_argmax_positive_count += inverse_argmax_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim_normal % : {cos_sim_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim_inverse_argmax % : {inverse_argmax_positive_count * 100 / total_count}\")\n",
    "\n",
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[10:30]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            inverse_argmax_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                    inverse_argmax_result[\"definition\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b5751",
   "metadata": {},
   "source": [
    "Последний тест -- аргмакс с препроцом и без:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fad16b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim_inverse_argmax no preproc % : 43.28358208955224\n",
      "cos_sim_inverse_argmax + preproc % : 41.791044776119406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    inverse_argmax_positive_count_no_preproc = 0\n",
    "    inverse_argmax_positive_count_w_preproc = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result, inverse_argmax_no_preproc_result, inverse_argmax_w_preproc_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "        inverse_argmax_positive_count_no_preproc += inverse_argmax_no_preproc_result == gold\n",
    "        inverse_argmax_positive_count_w_preproc += inverse_argmax_w_preproc_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim_inverse_argmax no preproc % : {inverse_argmax_positive_count_no_preproc * 100 / total_count}\")\n",
    "    print(f\"cos_sim_inverse_argmax + preproc % : {inverse_argmax_positive_count_w_preproc * 100 / total_count}\")\n",
    "\n",
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            inverse_argmax_no_preproc_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            inverse_argmax_w_preproc_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(Preprocessor.preprocess(context))\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    inverse_argmax_no_preproc_result[\"definition\"],\n",
    "                    inverse_argmax_w_preproc_result[\"definition\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572aea8a",
   "metadata": {},
   "source": [
    "Без препроца лучше, дальше весь датасет будем гнать в таком формате: жаккар + препроц, косинусное подобие без препроца. При этом сначала посчитаем жаккара отдельно для всего датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd3ec120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49453/49453 [02:13<00:00, 371.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 43.75961285966163\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "\n",
    "\n",
    "\n",
    "results = list()\n",
    "\n",
    "for sentence in tqdm(corpus):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af377d",
   "metadata": {},
   "source": [
    "И теперь посчитаем косинусное подобие отдельно для... ~~30 *вторых*~~ 100 предложений, потому что ~~мой компьютер не выдержит 140 часов расчетов~~ даже на хорошей карточке считает достаточно долго"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed7f0381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim % : 42.893081761006286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    cos_sim_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, cos_sim_result = result\n",
    "\n",
    "        cos_sim_positive_count += cos_sim_result == gold\n",
    "\n",
    "    print(f\"cos_sim % : {cos_sim_positive_count * 100 / total_count}\")\n",
    "\n",
    "\n",
    "results = list()\n",
    "\n",
    "# for sentence in tqdm(corpus[30:60]):\n",
    "for sentence in tqdm(corpus[:100]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11ac21",
   "metadata": {},
   "source": [
    "А тут получилось ~~лучше, чем на жаккаре, мб потому, что выборка маленькая.~~ так же. В любом случае, эксперимент заканчиваю."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2 (4 балла)\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "714095f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"./data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "995a0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5700402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bcbfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ARI(grouped_df, cluster):\n",
    "\n",
    "    ARI = []\n",
    "\n",
    "    for key, _ in grouped_df:\n",
    "        # вытаскиваем контексты\n",
    "        texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "        # создаем пустую матрицу для векторов \n",
    "        X = np.zeros((len(texts), 768))\n",
    "\n",
    "        # переводим тексты в векторы и кладем в матрицу\n",
    "        for i, text in enumerate(texts):\n",
    "            X[i] = model.encode(text)\n",
    "\n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "        # расчитываем метрику для отдельного слова\n",
    "        ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "        \n",
    "    print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f81dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clusters': 8, 'max_iter': 300, 'algorithm': 'lloyd'}\t0.046061661243416444\n",
      "{'n_clusters': 8, 'max_iter': 300, 'algorithm': 'elkan'}\t0.035343829699609156\n",
      "{'n_clusters': 3, 'max_iter': 200, 'algorithm': 'lloyd'}\t0.045589545730551066\n",
      "{'n_clusters': 3, 'max_iter': 500, 'algorithm': 'lloyd'}\t0.026788975182023136\n",
      "{'n_clusters': 3, 'max_iter': 500, 'algorithm': 'elkan'}\t0.10632515799415188\n",
      "{'n_clusters': 6, 'max_iter': 300, 'algorithm': 'elkan'}\t0.04001386670575718\n"
     ]
    }
   ],
   "source": [
    "kmeans_params = (\n",
    "    {\n",
    "        \"n_clusters\": 8,\n",
    "        \"max_iter\": 300,\n",
    "        \"algorithm\":\"lloyd\"\n",
    "    },\n",
    "    {\n",
    "        \"n_clusters\": 8,\n",
    "        \"max_iter\": 300,\n",
    "        \"algorithm\":\"elkan\"\n",
    "    },\n",
    "    {\n",
    "        \"n_clusters\": 3,\n",
    "        \"max_iter\": 200,\n",
    "        \"algorithm\":\"lloyd\"\n",
    "    },\n",
    "    {\n",
    "        \"n_clusters\": 3,\n",
    "        \"max_iter\": 500,\n",
    "        \"algorithm\":\"lloyd\"\n",
    "    },\n",
    "    {\n",
    "        \"n_clusters\": 3,\n",
    "        \"max_iter\": 500,\n",
    "        \"algorithm\":\"elkan\"\n",
    "    },\n",
    "    {\n",
    "        \"n_clusters\": 6,\n",
    "        \"max_iter\": 300,\n",
    "        \"algorithm\":\"elkan\"\n",
    "    },\n",
    ")\n",
    "\n",
    "for params in kmeans_params:\n",
    "    print(params, end=\"\\t\")\n",
    "    kmeans = KMeans(**params)\n",
    "    get_ARI(grouped_df, kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb52c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'damping': 0.5, 'max_iter': 200, 'affinity': 'euclidean', 'convergence_iter': 15}\t0.042740969848549505\n",
      "{'damping': 0.9, 'max_iter': 200, 'affinity': 'euclidean', 'convergence_iter': 15}\t0.05297560306165972\n",
      "{'damping': 0.9, 'max_iter': 100, 'affinity': 'euclidean', 'convergence_iter': 45}\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/sklearn/cluster/_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n",
      "/home/smertlove/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/sklearn/cluster/_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n",
      "/home/smertlove/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/sklearn/cluster/_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04150923434928373\n",
      "{'damping': 0.5, 'max_iter': 200, 'affinity': 'euclidean', 'convergence_iter': 20}\t0.042740969848549505\n",
      "{'damping': 0.8, 'max_iter': 300, 'affinity': 'euclidean', 'convergence_iter': 25}\t0.04154515818974152\n",
      "{'damping': 0.5, 'max_iter': 350, 'affinity': 'euclidean', 'convergence_iter': 35}\t0.042740969848549505\n"
     ]
    }
   ],
   "source": [
    "affinity_params = (\n",
    "    {\n",
    "        \"damping\": 0.5,\n",
    "        \"max_iter\": 200,\n",
    "        \"affinity\":\"euclidean\",\n",
    "        \"convergence_iter\": 15,\n",
    "    },\n",
    "    {\n",
    "        \"damping\": 0.9,\n",
    "        \"max_iter\": 200,\n",
    "        \"affinity\":\"euclidean\",\n",
    "        \"convergence_iter\": 15,\n",
    "    },\n",
    "    {\n",
    "        \"damping\": 0.9,\n",
    "        \"max_iter\": 100,\n",
    "        \"affinity\":\"euclidean\",\n",
    "        \"convergence_iter\": 45,\n",
    "    },\n",
    "    {\n",
    "        \"damping\": 0.5,\n",
    "        \"max_iter\": 200,\n",
    "        \"affinity\":\"euclidean\",\n",
    "        \"convergence_iter\": 20,\n",
    "    },\n",
    "    {\n",
    "        \"damping\": 0.8,\n",
    "        \"max_iter\": 300,\n",
    "        \"affinity\":\"euclidean\",\n",
    "        \"convergence_iter\": 25,\n",
    "    },\n",
    "    {\n",
    "        \"damping\": 0.5,\n",
    "        \"max_iter\": 350,\n",
    "        \"affinity\":\"euclidean\",\n",
    "        \"convergence_iter\": 35,\n",
    "    },\n",
    ")\n",
    "\n",
    "for params in affinity_params:\n",
    "    print(params, end=\"\\t\")\n",
    "    affinity = AffinityPropagation(**params)\n",
    "    get_ARI(grouped_df, affinity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69390953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eps': 0.5, 'min_samples': 5}\t-0.011271692824715207\n",
      "{'eps': 0.3, 'min_samples': 3}\t-0.031138266011086928\n",
      "{'eps': 0.8, 'min_samples': 8}\t-0.001784882962278153\n",
      "{'eps': 0.3, 'min_samples': 8}\t0.0\n",
      "{'eps': 0.8, 'min_samples': 3}\t-0.001784882962278153\n",
      "{'eps': 0.2, 'min_samples': 2}\t0.007404278375247722\n"
     ]
    }
   ],
   "source": [
    "dbscan_params = (\n",
    "    {\n",
    "    \"eps\": 0.5,\n",
    "    \"min_samples\":5,\n",
    "    },\n",
    "    {\n",
    "    \"eps\": 0.3,\n",
    "    \"min_samples\":3,\n",
    "    },\n",
    "    {\n",
    "    \"eps\": 0.8,\n",
    "    \"min_samples\":8,\n",
    "    },\n",
    "    {\n",
    "    \"eps\": 0.3,\n",
    "    \"min_samples\":8,\n",
    "    },\n",
    "    {\n",
    "    \"eps\": 0.8,\n",
    "    \"min_samples\":3,\n",
    "    },\n",
    "    {\n",
    "    \"eps\": 0.2,\n",
    "    \"min_samples\":2,\n",
    "    },\n",
    ")\n",
    "\n",
    "for params in dbscan_params:\n",
    "    print(params, end=\"\\t\")\n",
    "    dbscan = DBSCAN(**params)\n",
    "    get_ARI(grouped_df, dbscan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a39b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f32a3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 2\t-0.011976265536517934\n",
      "n_clusters: 3\t0.034738071269679226\n",
      "n_clusters: 4\t0.038617257915792846\n",
      "n_clusters: 5\t0.03379296274962468\n",
      "n_clusters: 6\t0.03957581346650506\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 7):\n",
    "    print(f\"n_clusters: {i}\", end=\"\\t\")\n",
    "    agglomerative = AgglomerativeClustering(i)\n",
    "    get_ARI(grouped_df, agglomerative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db08ce0",
   "metadata": {},
   "source": [
    "##  HDBSCAN\n",
    "\n",
    "Тот же DBSCAN, но с эвристикой: делается предположение, что эпсилон-окрестность однородна на всей выборке. Автоматически исследуются различные эпсилон-окрестности, выбирается подходящая. Таким образом, появляется возможность собирать кластеры разной плотности на одной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e4b6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4072b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 2\t0.008236037742083282\n",
      "n_clusters: 6\t-0.013894256669353344\n",
      "n_clusters: 10\t-0.02876603736237084\n",
      "n_clusters: 14\t0.0\n",
      "n_clusters: 18\t0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 22, 4):\n",
    "    print(f\"n_clusters: {i}\", end=\"\\t\") \n",
    "    hdbscan = HDBSCAN(min_samples=i)\n",
    "    get_ARI(grouped_df, hdbscan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
