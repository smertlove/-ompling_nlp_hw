{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1 (4 балла)\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05357a75",
   "metadata": {},
   "source": [
    "## Решение:\n",
    "\n",
    "### Загрузка датасета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5814a100-9ba8-4787-b0ac-73fa657a3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "corpus_path = pathlib.Path(r\"data/corpus_wsd_50k.txt\")\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.read().split('\\n\\n')\n",
    "    corpus = map(\n",
    "        lambda sent: [\n",
    "            word.split(\"\\t\")\n",
    "            for word\n",
    "            in sent.split(\"\\n\")\n",
    "            ],\n",
    "        corpus\n",
    "        )\n",
    "    corpus = tuple(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b009fee",
   "metadata": {},
   "source": [
    "###  Загрузка ворднета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbeefc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kirill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f90e0d",
   "metadata": {},
   "source": [
    "###  Функция, составляющая контекст\n",
    "\n",
    "Конетекстом считаем предложение, в котором встречается слово, без самого слова. Будем брать предложение из датасета целиком и маскировать в нем слово, для которого нужно определить значение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc38e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask_target(corpus_sentence:list, target_ind:int) -> list:\n",
    "\n",
    "    result = list()\n",
    "\n",
    "    for ind, word in enumerate(corpus_sentence):\n",
    "        if ind != target_ind:\n",
    "            result.append(word[1])\n",
    "        else:\n",
    "            result.append(\"<mask/>\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c98d1",
   "metadata": {},
   "source": [
    "### Препроц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa6d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    stopwords:list[str] = nltk.corpus.stopwords.words(\"english\")\n",
    "    stopwords.extend(punctuation)\n",
    "    stopwords = set(stopwords)\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text:str) -> list[str]:\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "    @classmethod\n",
    "    def rm_stopwords(cls, tokenized_text:list[str]) -> list[str]:\n",
    "        return [c for c in tokenized_text if c not in cls.stopwords]\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, text: str | list[str]) -> list[str]:\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text = cls.tokenize(text)\n",
    "        \n",
    "        text = [c.lower() for c in text]      \n",
    "        text = cls.rm_stopwords(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808a2a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['permit',\n",
       " 'become',\n",
       " 'giveaway',\n",
       " '<mask/>',\n",
       " 'rather',\n",
       " 'one',\n",
       " 'goal',\n",
       " 'improved',\n",
       " 'employee',\n",
       " 'morale',\n",
       " 'consequently',\n",
       " 'increased',\n",
       " 'productivity']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверяем препроц\n",
    "\n",
    "s = corpus[1]\n",
    "s = get_context_mask_target(s, 8)\n",
    "s = Preprocessor.preprocess(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c62ab",
   "metadata": {},
   "source": [
    "###  Алгоритм Леска в общем виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43f96fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lesk:\n",
    "\n",
    "    #  Псевдометрика, переопределяется в классах-наследниках\n",
    "    metric = lambda sent1, sent2: 0\n",
    "\n",
    "    @classmethod\n",
    "    def __get_all_definitions(cls, word:str) -> str:\n",
    "        return [c.definition() for c in wn.synsets(word)]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_definition(cls, word:str, context:list[str]) -> str:\n",
    "\n",
    "        all_results = [\n",
    "            {\n",
    "                \"definition\": definition,\n",
    "                \"score\"     : cls.metric(context, Preprocessor.preprocess(definition))\n",
    "            }\n",
    "            for definition\n",
    "            in cls.__get_all_definitions(word)\n",
    "        ]\n",
    "\n",
    "        return max(all_results, key=lambda c: c[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3bc82",
   "metadata": {},
   "source": [
    "### Алгоритм Леска с метрикой Жаккара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd31c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(sent1:list[str], sent2:list[str]) -> float:\n",
    "    sent1 = set(sent1)\n",
    "    sent2 = set(sent2)\n",
    "\n",
    "    return len(sent1 & sent2) / len(sent1 | sent2)\n",
    "\n",
    "\n",
    "class LeskWithJaccard(Lesk):\n",
    "\n",
    "    metric = jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e4e9ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant',\n",
       " 'score': 0.2727272727272727}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my car with my personnel in it to my dads power plant.\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithJaccard.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eba7f63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a motor vehicle with four wheels; usually propelled by an internal combustion engine',\n",
       " 'score': 0.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW car from the shop and it's amazing\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithJaccard.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90288f5",
   "metadata": {},
   "source": [
    "### ### Алгоритм Леска с косинусной близостью на эмбедингах sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec2ca660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ce9faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSE subj data\\актуальные проблемы компьютерной лингвистики\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebbb790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(sent1:list[str], sent2:list[str]) -> float:\n",
    "    emb1 = model.encode(\" \".join(sent1)).reshape(1, -1)\n",
    "    emb2 = model.encode(\" \".join(sent2)).reshape(1, -1)\n",
    "\n",
    "    return float(cosine_distances(emb1, emb2)[0][0])\n",
    "\n",
    "class LeskWithCosSim(Lesk):\n",
    "\n",
    "    metric = cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bb29206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'where passengers ride up and down',\n",
       " 'score': 0.8298293352127075}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my car with my personnel in it to my dads power plant.\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3585c07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a wheeled vehicle adapted to the rails of railroad',\n",
       " 'score': 0.932273805141449}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW car from the shop and it's amazing\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2 (4 балла)\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714095f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
