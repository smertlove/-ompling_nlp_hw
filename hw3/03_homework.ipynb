{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1 (4 балла)\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05357a75",
   "metadata": {},
   "source": [
    "## Решение:\n",
    "\n",
    "*NOTE: я не знаю, как умудрился получить такие результаты. Прошу прощения за полотна дублированного кода.*\n",
    "\n",
    "### Загрузка датасета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5814a100-9ba8-4787-b0ac-73fa657a3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "corpus_path = pathlib.Path(r\"data/corpus_wsd_50k.txt\")\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.read().split('\\n\\n')\n",
    "    corpus = map(\n",
    "        lambda sent: [\n",
    "            word.split(\"\\t\")\n",
    "            for word\n",
    "            in sent.split(\"\\n\")\n",
    "            ],\n",
    "        corpus\n",
    "        )\n",
    "    corpus = tuple(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b009fee",
   "metadata": {},
   "source": [
    "###  Загрузка ворднета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cbeefc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kirill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f90e0d",
   "metadata": {},
   "source": [
    "###  Функция, составляющая контекст\n",
    "\n",
    "Конетекстом считаем предложение, в котором встречается слово, без самого слова. Будем брать предложение из датасета целиком и маскировать в нем слово, для которого нужно определить значение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc38e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask_target(corpus_sentence:list, target_ind:int) -> list:\n",
    "\n",
    "    result = list()\n",
    "\n",
    "    for ind, word in enumerate(corpus_sentence):\n",
    "        if ind != target_ind:\n",
    "            result.append(word[1])\n",
    "        else:\n",
    "            result.append(\"_\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c98d1",
   "metadata": {},
   "source": [
    "### Препроц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afa6d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    stopwords:list[str] = nltk.corpus.stopwords.words(\"english\")\n",
    "    stopwords.extend(punctuation)\n",
    "    stopwords.append(\"_\")\n",
    "    stopwords = set(stopwords)\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text:str) -> list[str]:\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "    @classmethod\n",
    "    def rm_stopwords(cls, tokenized_text:list[str]) -> list[str]:\n",
    "        return [c for c in tokenized_text if c not in cls.stopwords]\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, text: str | list[str]) -> list[str]:\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text = cls.tokenize(text)\n",
    "        \n",
    "        text = [c.lower() for c in text]      \n",
    "        text = cls.rm_stopwords(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "808a2a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['permit',\n",
       " 'become',\n",
       " 'giveaway',\n",
       " 'rather',\n",
       " 'one',\n",
       " 'goal',\n",
       " 'improved',\n",
       " 'employee',\n",
       " 'morale',\n",
       " 'consequently',\n",
       " 'increased',\n",
       " 'productivity']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверяем препроц\n",
    "\n",
    "s = corpus[1]\n",
    "s = get_context_mask_target(s, 8)\n",
    "s = Preprocessor.preprocess(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c62ab",
   "metadata": {},
   "source": [
    "###  Алгоритм Леска в общем виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "43f96fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lesk:\n",
    "\n",
    "    #  Псевдометрика, переопределяется в классах-наследниках\n",
    "    metric = lambda sent1, sent2: 0\n",
    "\n",
    "    @classmethod\n",
    "    def _get_all_definitions(cls, word:str) -> str:\n",
    "        return [c.definition() for c in wn.synsets(word)]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_definition(cls, word:str, context:list[str]) -> str:\n",
    "\n",
    "        all_results = [\n",
    "            {\n",
    "                \"definition\": definition,\n",
    "                \"score\"     : cls.metric(context, Preprocessor.preprocess(definition))\n",
    "            }\n",
    "            for definition\n",
    "            in cls._get_all_definitions(word)\n",
    "        ]\n",
    "\n",
    "        return max(all_results, key=lambda c: c[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3bc82",
   "metadata": {},
   "source": [
    "### Алгоритм Леска с метрикой Жаккара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bd31c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(sent1:list[str], sent2:list[str]) -> float:\n",
    "    sent1 = set(sent1)\n",
    "    sent2 = set(sent2)\n",
    "\n",
    "    return len(sent1 & sent2) / len(sent1 | sent2)\n",
    "\n",
    "\n",
    "class LeskWithJaccard(Lesk):\n",
    "    metric = jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3e4e9ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant',\n",
       " 'score': 0.3}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my _ with my personnel in it to my dads power plant.\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithJaccard.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "eba7f63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a motor vehicle with four wheels; usually propelled by an internal combustion engine',\n",
       " 'score': 0.0}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW _ from the shop and it's amazing\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithJaccard.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90288f5",
   "metadata": {},
   "source": [
    "### Алгоритм Леска с косинусной близостью на эмбедингах sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ec2ca660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5ce9faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSE subj data\\актуальные проблемы компьютерной лингвистики\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ebbb790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(sent1:list[str], sent2:list[str]) -> float:\n",
    "    emb1 = model.encode(\" \".join(sent1)).reshape(1, -1)\n",
    "    emb2 = model.encode(\" \".join(sent2)).reshape(1, -1)\n",
    "\n",
    "    return float(cosine_distances(emb1, emb2)[0][0])\n",
    "\n",
    "class LeskWithCosSim(Lesk):\n",
    "    metric = cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "3bb29206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'where passengers ride up and down',\n",
       " 'score': 0.8515400886535645}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my _ with my personnel in it to my dads power plant.\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3585c07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a wheeled vehicle adapted to the rails of railroad',\n",
       " 'score': 0.928614616394043}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW _ from the shop and it's amazing\"\n",
    "ctx = Preprocessor.preprocess(ctx)\n",
    "\n",
    "LeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828ae0b",
   "metadata": {},
   "source": [
    "### Всё готово для сравнительного тестирования\n",
    "\n",
    "Осталось пройти по всему датасету, предсказать значения для всех многозначных слов обоими способами и сравнить результаты с эталонными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "71ced77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "542dc619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:08<00:00, 24.81s/it]\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                context\n",
    "            )\n",
    "\n",
    "            cos_sim_result = LeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e04ec",
   "metadata": {},
   "source": [
    "Первые 10 предложений в корпусе обрабатываются 4 с лишним минуты. Что вдвойне обидно, если принять во внимание тот факт, что в коде выше я забыл записать результаты замеров. Тем не менее, есть подозрение, что функция расчета косинусного подобия работает слишком медленно. Ниже попробую оптимизировать ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "55a7192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "##  Тут поменяем интерфейс так, чтобы не было лишних операций str.join\n",
    "##  и заменим функцию расчета косинусной близости, чтобы модель кодировала контекст только 1 раз.\n",
    "def better_cos_sim(context_sent:str, other_sents:list[str]) -> np.ndarray:\n",
    "    context_emb = model.encode(context_sent).reshape(1, -1)\n",
    "    other_embs = [\n",
    "        model.encode(sent)\n",
    "        for sent\n",
    "        in other_sents\n",
    "    ]\n",
    "    return cosine_distances(context_emb, other_embs)[0]\n",
    "\n",
    "\n",
    "##  Тут поменяем логику под новый расчет метрики\n",
    "class BetterLeskWithCosSim(Lesk):\n",
    "    \n",
    "    metric = better_cos_sim\n",
    "\n",
    "    @classmethod\n",
    "    def get_definition(cls, word:str, context:str) -> str:\n",
    "\n",
    "        definitions = cls._get_all_definitions(word)\n",
    "        metrics: np.ndarray = cls.metric(context, definitions)\n",
    "\n",
    "        argmax = metrics.argmax()\n",
    "\n",
    "        return {\n",
    "            \"definition\": definitions[argmax],\n",
    "            \"score\"     : float(metrics[argmax])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "0e21c9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant',\n",
       " 'score': 0.7972332835197449}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Проверка:\n",
    "\n",
    "word = \"car\"\n",
    "ctx = \"I drive my _ with my personnel in it to my dads power plant.\"\n",
    "ctx = \" \".join(Preprocessor.preprocess(ctx))\n",
    "\n",
    "BetterLeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5f0c77a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definition': 'a conveyance for passengers or freight on a cable railway',\n",
       " 'score': 0.9488925933837891}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"car\"\n",
    "ctx = \"I have just bought a new BMW _ from the shop and it's amazing\"\n",
    "ctx = \" \".join(Preprocessor.preprocess(ctx))\n",
    "\n",
    "BetterLeskWithCosSim.get_definition(word, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b871014",
   "metadata": {},
   "source": [
    "Результаты другие. Возможно, изначально метрика была посчитана неверно. В любом случае пока что кажется, что имеется неплохое ускорение. Ниже дублирую код замеров, не забыв при этом записать результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "16d2561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.60s/it]\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "            context = Preprocessor.preprocess(context)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                context\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1b0d1",
   "metadata": {},
   "source": [
    "Четырехкратное ускорение, это победа. Отпечатаем результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "bc8231c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    cos_sim_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result, cos_sim_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "        cos_sim_positive_count += cos_sim_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim % : {cos_sim_positive_count * 100 / total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "83682eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim % : 14.925373134328359\n"
     ]
    }
   ],
   "source": [
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623986fe",
   "metadata": {},
   "source": [
    "Ускорение получили, а вот нормально работать в сделку уже не входило. При чем если с жаккаром такие результаты еще можно понять, то с косинусным подобием точно что-то пошло не так. Попробуем убрать препроц в расчете метрики косинусного подобия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8ed399dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:20<00:00,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim % : 19.402985074626866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bcf76",
   "metadata": {},
   "source": [
    "Уже лучше, но все еще плохо. Попробуем вместо максимального значения подобия взять минимальное:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e7b69a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterLeskWithCosSim_InverseArgmax(Lesk):\n",
    "    \n",
    "    metric = better_cos_sim\n",
    "\n",
    "    @classmethod\n",
    "    def get_definition(cls, word:str, context:str) -> str:\n",
    "\n",
    "        definitions = cls._get_all_definitions(word)\n",
    "        metrics: np.ndarray = cls.metric(context, definitions)\n",
    "        \n",
    "        #  :o)\n",
    "        argmax = metrics.argmin()\n",
    "\n",
    "        return {\n",
    "            \"definition\": definitions[argmax],\n",
    "            \"score\"     : float(metrics[argmax])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1ca3c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:19<00:00,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim % : 43.28358208955224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083653af",
   "metadata": {},
   "source": [
    "Результаты неожиданные. Посмотрим, что посчитается на следующих 20 предложениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7783c3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:36<00:00, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 54.30107526881721\n",
      "cos_sim_normal % : 24.193548387096776\n",
      "cos_sim_inverse_argmax % : 40.32258064516129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    cos_sim_positive_count = 0\n",
    "    inverse_argmax_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result, cos_sim_result, inverse_argmax_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "        cos_sim_positive_count += cos_sim_result == gold\n",
    "        inverse_argmax_positive_count += inverse_argmax_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim_normal % : {cos_sim_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim_inverse_argmax % : {inverse_argmax_positive_count * 100 / total_count}\")\n",
    "\n",
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[10:30]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            inverse_argmax_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                    inverse_argmax_result[\"definition\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b5751",
   "metadata": {},
   "source": [
    "Последний тест -- аргмакс с препроцом и без:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "fad16b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:27<00:00, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 56.71641791044776\n",
      "cos_sim_inverse_argmax no preproc % : 43.28358208955224\n",
      "cos_sim_inverse_argmax + preproc % : 41.791044776119406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    inverse_argmax_positive_count_no_preproc = 0\n",
    "    inverse_argmax_positive_count_w_preproc = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result, inverse_argmax_no_preproc_result, inverse_argmax_w_preproc_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "        inverse_argmax_positive_count_no_preproc += inverse_argmax_no_preproc_result == gold\n",
    "        inverse_argmax_positive_count_w_preproc += inverse_argmax_w_preproc_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "    print(f\"cos_sim_inverse_argmax no preproc % : {inverse_argmax_positive_count_no_preproc * 100 / total_count}\")\n",
    "    print(f\"cos_sim_inverse_argmax + preproc % : {inverse_argmax_positive_count_w_preproc * 100 / total_count}\")\n",
    "\n",
    "results = list()\n",
    "\n",
    "\n",
    "for sentence in tqdm(corpus[:10]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            inverse_argmax_no_preproc_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            inverse_argmax_w_preproc_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(Preprocessor.preprocess(context))\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                    inverse_argmax_no_preproc_result[\"definition\"],\n",
    "                    inverse_argmax_w_preproc_result[\"definition\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572aea8a",
   "metadata": {},
   "source": [
    "Без препроца лучше, дальше весь датасет будем гнать в таком формате: жаккар + препроц, косинусное подобие без препроца. При этом сначала посчитаем жаккара отдельно для всего датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cd3ec120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49453 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49453/49453 [05:06<00:00, 161.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard % : 43.75961285966163\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    jaccard_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, jaccard_result = result\n",
    "\n",
    "        jaccard_positive_count += jaccard_result == gold\n",
    "\n",
    "    print(f\"jaccard % : {jaccard_positive_count * 100 / total_count}\")\n",
    "\n",
    "\n",
    "\n",
    "results = list()\n",
    "\n",
    "for sentence in tqdm(corpus):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            jaccard_result = LeskWithJaccard.get_definition(\n",
    "                word[1],\n",
    "                Preprocessor.preprocess(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    jaccard_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af377d",
   "metadata": {},
   "source": [
    "И теперь посчитаем косинусное подобие отдельно для... 30 *вторых* предложений, потому что мой компьютер не выдержит 140 часов расчетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ed7f0381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:32<00:00,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim % : 47.27272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_results(results:list) -> None:\n",
    "\n",
    "    cos_sim_positive_count = 0\n",
    "    total_count = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        gold, cos_sim_result = result\n",
    "\n",
    "        cos_sim_positive_count += cos_sim_result == gold\n",
    "\n",
    "    print(f\"cos_sim % : {cos_sim_positive_count * 100 / total_count}\")\n",
    "\n",
    "\n",
    "results = list()\n",
    "\n",
    "for sentence in tqdm(corpus[30:60]):\n",
    "    for i, word in enumerate(sentence):\n",
    "\n",
    "        if not word[0].strip():  ##  Пропуск однозначного слова\n",
    "            continue\n",
    "\n",
    "        else:  ##  Обработка многозначного слова\n",
    "            gold = wn.lemma_from_key(word[0]).synset().definition()\n",
    "\n",
    "            context = get_context_mask_target(sentence, i)\n",
    "\n",
    "            cos_sim_result = BetterLeskWithCosSim_InverseArgmax.get_definition(\n",
    "                word[1],\n",
    "                \" \".join(context)\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                (\n",
    "                    gold,\n",
    "                    cos_sim_result[\"definition\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "count_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11ac21",
   "metadata": {},
   "source": [
    "А тут получилось лучше, чем на жаккаре, мб потому, что выборка маленькая. В любом случае, эксперимент заканчиваю."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2 (4 балла)\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "714095f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"./data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "995a0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e5700402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8bcbfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ARI(grouped_df, cluster):\n",
    "\n",
    "    ARI = []\n",
    "\n",
    "    for key, _ in grouped_df:\n",
    "        # вытаскиваем контексты\n",
    "        texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "        # создаем пустую матрицу для векторов \n",
    "        X = np.zeros((len(texts), 768))\n",
    "\n",
    "        # переводим тексты в векторы и кладем в матрицу\n",
    "        for i, text in enumerate(texts):\n",
    "            X[i] = model.encode(text)\n",
    "\n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "        # расчитываем метрику для отдельного слова\n",
    "        ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "        \n",
    "    print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "2f81dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09010860802875485\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_ARI(grouped_df, KMeans(3))\n",
    "\n",
    "##  7 минут :(\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # выбираем один из алгоритмов\n",
    "# # cluster = AffinityPropagation(damping=0.9)\n",
    "# cluster = KMeans(3)\n",
    "# #     cluster = DBSCAN(min_samples=1, eps=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
