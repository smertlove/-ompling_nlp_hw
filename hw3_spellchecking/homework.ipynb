{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba19c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wiki_data.txt', encoding='utf8')as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a892f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import textdistance\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7345bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(re.findall(r'\\w+', corpus.lower()))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', max_features=10000, ngram_range=(1,3))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4aa7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
    "    # 1 вектора ко всей матрице (словам в словаре)\n",
    "    # считать по отдельности циклом было бы дольше\n",
    "    # вместо одного вектора может даже целая матрица\n",
    "    # тогда считаться в итоге будет ещё быстрее\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    # Counter можно использовать и с не целыми числами\n",
    "    similarities = Counter()\n",
    "    \n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    \n",
    "    return closest\n",
    "\n",
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    ##  доработка 1  ##\n",
    "    if text in vocab:\n",
    "        return text\n",
    "    ##  конец доработки 1  ##\n",
    "\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    ##  доработка 2  ##\n",
    "    max_dl = max([c[1] for c in closest])\n",
    "\n",
    "    closest = {\n",
    "        word: {\n",
    "            \"DL\": dist,\n",
    "            \"P\" : P(word),\n",
    "        }\n",
    "        for word, dist\n",
    "        in closest\n",
    "        if dist == max_dl\n",
    "    }\n",
    "\n",
    "    return max(closest, key=lambda c: closest[c][\"P\"])\n",
    "    ##  конец доработки 2  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e67f8d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'солнце'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_hybrid_match(\"cсолнце\", X, vec, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e2a96",
   "metadata": {},
   "source": [
    "#### Комментарий\n",
    "\n",
    "Для сравнения возьмем еще Норвига\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2417de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оригинальный код вот тут - https://norvig.com/spell-correct.html\n",
    "# я только адаптировал его под русский язык\n",
    "\n",
    "def correction(word):\n",
    "    closest = candidates(word)\n",
    "    closest = {\n",
    "        word: {\n",
    "            \"P\" : P(word),\n",
    "        }\n",
    "        for word\n",
    "        in closest\n",
    "    }\n",
    "    \"Находим наиболее вероятное похожее слово\"\n",
    "    return max(closest, key=lambda c: closest[c][\"P\"])\n",
    "\n",
    "def candidates(word): \n",
    "    \"Генерируем кандидатов на исправление\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"Выбираем слова, которые есть в корпусе\"\n",
    "    return set(w for w in words if w in vocab)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
    "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"Создаем кандидатов, которые отличаются на две буквы\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3145ead9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'солнце'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"cсолнце\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "060f5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "46b540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Tester:\n",
    "\n",
    "    bad = open('data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "    true = open('data/correct_sents.txt', encoding='utf8').read().splitlines()\n",
    "\n",
    "    @classmethod\n",
    "    def get_metrics(cls, correction_func: Callable) -> str:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        total_mistaken = 0\n",
    "        mistaken_fixed = 0\n",
    "\n",
    "        total_correct = 0\n",
    "        correct_broken = 0\n",
    "\n",
    "        cashed = {}\n",
    "        for i in tqdm(range(len(cls.true))):\n",
    "            word_pairs = align_words(cls.true[i], cls.bad[i])\n",
    "            for pair in word_pairs:\n",
    "                # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "                # перед тем как считать исправление проверим нет ли его в кеше\n",
    "                \n",
    "                predicted = cashed.get(pair[1], correction_func(pair[1]))\n",
    "                cashed[pair[1]] = predicted\n",
    "                \n",
    "                \n",
    "                if predicted == pair[0]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "                \n",
    "                if pair[0] == pair[1]:\n",
    "                    total_correct += 1\n",
    "                    if pair[0] !=  predicted:\n",
    "                        correct_broken += 1\n",
    "                else:\n",
    "                    total_mistaken += 1\n",
    "                    if pair[0] == predicted:\n",
    "                        mistaken_fixed += 1\n",
    "\n",
    "        return (\n",
    "            correct/total,\n",
    "            mistaken_fixed/total_mistaken,\n",
    "            correct_broken/total_correct,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6938c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 915/915 [07:55<00:00,  1.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.856328164082041, 0.4937888198757764, 0.09004249454461927)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester.get_metrics(\n",
    "    lambda word: get_closest_hybrid_match(word, X, vec, topn=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9f9ec4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/915 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 915/915 [01:22<00:00, 11.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.870935467733867, 0.5124223602484472, 0.07603077983231882)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester.get_metrics(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb76b8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (7 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Он основан только на одной операции - удалении символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово  (!) \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symspell:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_deletes(word: str) -> set[str]:\n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = {L + R[1:]            for L, R in splits if R}\n",
    "        return deletes\n",
    "\n",
    "    del_dict: dict[str, str] = { }\n",
    "    for word in sorted(tuple(vocab), key=lambda c: vocab[c]):  ##  так в словаре окажутся наиболее частотные варианты\n",
    "        for delete in get_deletes(word):\n",
    "            del_dict[delete] = word\n",
    "        del_dict[word] = word\n",
    "    del_set = set(del_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def correct(cls, word:str) -> str:\n",
    "\n",
    "        if word in vocab:\n",
    "            return word\n",
    "\n",
    "        deletes     : set[str]   = cls.get_deletes(word)\n",
    "        intersection: tuple[str] = tuple(deletes & cls.del_set)\n",
    "\n",
    "        inter_len = len(intersection)\n",
    "        if inter_len == 0:\n",
    "            return word\n",
    "        elif inter_len == 1:\n",
    "            return cls.del_dict[intersection[0]]\n",
    "        else:\n",
    "            return max([cls.del_dict[w] for w in intersection], key=P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "954e70bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'солнце'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Symspell.correct(\"cсолнце\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d8398873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 915/915 [00:00<00:00, 35244.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8666333166583292, 0.30434782608695654, 0.05018950269897783)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester.get_metrics(\n",
    "    Symspell.correct\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2a6b6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symspell:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_deletes(word: str) -> set[str]:\n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = {L + R[1:]            for L, R in splits if R}\n",
    "        return deletes\n",
    "\n",
    "    del_dict: dict[str, str] = { }\n",
    "    for word in vocab:  ##  так в словаре окажутся наиболее частотные варианты\n",
    "        for delete in get_deletes(word):\n",
    "            del_dict[delete] = word\n",
    "        del_dict[word] = word\n",
    "    del_set = set(del_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def correct(cls, word:str) -> str:\n",
    "\n",
    "        if word in vocab:\n",
    "            return word\n",
    "\n",
    "        deletes     : set[str]   = cls.get_deletes(word)\n",
    "        intersection: tuple[str] = tuple(deletes & cls.del_set)\n",
    "\n",
    "        inter_len = len(intersection)\n",
    "        if inter_len == 0:\n",
    "            return word\n",
    "        elif inter_len == 1:\n",
    "            return cls.del_dict[intersection[0]]\n",
    "        else:\n",
    "            return max([cls.del_dict[w] for w in intersection], key=P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5006d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.get_metrics(\n",
    "    Symspell.correct\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211e934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
