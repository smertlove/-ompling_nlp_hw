{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc8290",
   "metadata": {},
   "source": [
    "## Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a5ace",
   "metadata": {},
   "source": [
    "Посчитайте частоты для 5-грамм в корпусе lenta.txt. двумя способами:  \n",
    "1) lenta.txt -> sent_tokenize (russian) -> word_tokenize -> ngrammer  \n",
    "2) lenta.txt -> word_tokene(preserve_line=True) - ngrammer  \n",
    "    \n",
    "Проанализируйте топ-20 самых частотных нграмм и проверьте есть ли различия? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfe6a96-b23b-41a0-b7b9-1d9f974dfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/lenta.txt\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "957f5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe4951e-7406-4601-a943-71b87b3b1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = sent_tokenize(data, language='russian')\n",
    "sentences1 = [word_tokenize(sentence) for sentence in sentences1]\n",
    "sentences1 = [\n",
    "    [token.lower() for token in sentence if not re.match(r'\\W+', token)] \n",
    "    for sentence in sentences1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0566988d-6d3d-4636-bc05-1cfa589a44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def get_20_most_common(sentences, n):\n",
    "    counter = Counter()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        counter.update(ngrammer(sentence, n))\n",
    "    \n",
    "    return counter.most_common(20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83c72eb3-96a2-46fb-bdde-910290e306aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('риа новости со ссылкой на', 400),\n",
       " ('сообщает риа новости со ссылкой', 320),\n",
       " ('как сообщили риа новости в', 196),\n",
       " ('как сообщает риа новости со', 149),\n",
       " ('сообщает интерфакс со ссылкой на', 142),\n",
       " ('сообщает итар-тасс со ссылкой на', 118),\n",
       " ('об этом риа новости сообщили', 113),\n",
       " ('об этом сообщает риа новости', 104),\n",
       " ('этом риа новости сообщили в', 99),\n",
       " ('со ссылкой на источники в', 93),\n",
       " ('сообщили риа новости в пресс-службе', 88),\n",
       " ('группировки войск на северном кавказе', 84),\n",
       " ('как сообщает интерфакс со ссылкой', 83),\n",
       " ('объединенной группировки войск на северном', 83),\n",
       " ('новости со ссылкой на пресс-службу', 76),\n",
       " ('эхо москвы со ссылкой на', 76),\n",
       " ('этом сообщает риа новости со', 75),\n",
       " ('в связи с тем что', 70),\n",
       " ('по борьбе с организованной преступностью', 66),\n",
       " ('как сообщает итар-тасс со ссылкой', 58)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = get_20_most_common(sentences1, 5)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075d7235-62df-46ec-8a2e-a0fbfa6af1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = word_tokenize(data, preserve_line=True)\n",
    "sentences2 = [token.lower() for token in sentences2 if not re.match(r'\\W+', token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4583494-e6b5-469c-a029-8448ec45706d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('риа новости со ссылкой на', 400),\n",
       " ('сообщает риа новости со ссылкой', 320),\n",
       " ('как сообщили риа новости в', 196),\n",
       " ('как сообщает риа новости со', 149),\n",
       " ('сообщает интерфакс со ссылкой на', 142),\n",
       " ('сообщает итар-тасс со ссылкой на', 118),\n",
       " ('об этом риа новости сообщили', 113),\n",
       " ('об этом сообщает риа новости', 104),\n",
       " ('этом риа новости сообщили в', 99),\n",
       " ('со ссылкой на источники в', 93),\n",
       " ('сообщили риа новости в пресс-службе', 88),\n",
       " ('как сообщает интерфакс со ссылкой', 83),\n",
       " ('объединенной группировки войск на северном', 83),\n",
       " ('эхо москвы со ссылкой на', 77),\n",
       " ('новости со ссылкой на пресс-службу', 76),\n",
       " ('этом сообщает риа новости со', 75),\n",
       " ('в связи с тем что', 70),\n",
       " ('как сообщает итар-тасс со ссылкой', 58),\n",
       " ('группировки войск на северном кавказе', 57),\n",
       " ('по борьбе с организованной преступностью', 55)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = get_20_most_common([sentences2], 5)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5865a163-ba32-451d-beec-2be4cea5f1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'риа новости со ссылкой на'\t'риа новости со ссылкой на'\tнграмма совпадает\n",
      "400\t400\tколичество совпадает\n",
      "\n",
      "'сообщает риа новости со ссылкой'\t'сообщает риа новости со ссылкой'\tнграмма совпадает\n",
      "320\t320\tколичество совпадает\n",
      "\n",
      "'как сообщили риа новости в'\t'как сообщили риа новости в'\tнграмма совпадает\n",
      "196\t196\tколичество совпадает\n",
      "\n",
      "'как сообщает риа новости со'\t'как сообщает риа новости со'\tнграмма совпадает\n",
      "149\t149\tколичество совпадает\n",
      "\n",
      "'сообщает интерфакс со ссылкой на'\t'сообщает интерфакс со ссылкой на'\tнграмма совпадает\n",
      "142\t142\tколичество совпадает\n",
      "\n",
      "'сообщает итар-тасс со ссылкой на'\t'сообщает итар-тасс со ссылкой на'\tнграмма совпадает\n",
      "118\t118\tколичество совпадает\n",
      "\n",
      "'об этом риа новости сообщили'\t'об этом риа новости сообщили'\tнграмма совпадает\n",
      "113\t113\tколичество совпадает\n",
      "\n",
      "'об этом сообщает риа новости'\t'об этом сообщает риа новости'\tнграмма совпадает\n",
      "104\t104\tколичество совпадает\n",
      "\n",
      "'этом риа новости сообщили в'\t'этом риа новости сообщили в'\tнграмма совпадает\n",
      "99\t99\tколичество совпадает\n",
      "\n",
      "'со ссылкой на источники в'\t'со ссылкой на источники в'\tнграмма совпадает\n",
      "93\t93\tколичество совпадает\n",
      "\n",
      "'сообщили риа новости в пресс-службе'\t'сообщили риа новости в пресс-службе'\tнграмма совпадает\n",
      "88\t88\tколичество совпадает\n",
      "\n",
      "'группировки войск на северном кавказе'\t'как сообщает интерфакс со ссылкой'\tнграмма не совпадает\n",
      "84\t83\tколичество не совпадает\n",
      "\n",
      "'как сообщает интерфакс со ссылкой'\t'объединенной группировки войск на северном'\tнграмма не совпадает\n",
      "83\t83\tколичество совпадает\n",
      "\n",
      "'объединенной группировки войск на северном'\t'эхо москвы со ссылкой на'\tнграмма не совпадает\n",
      "83\t77\tколичество не совпадает\n",
      "\n",
      "'новости со ссылкой на пресс-службу'\t'новости со ссылкой на пресс-службу'\tнграмма совпадает\n",
      "76\t76\tколичество совпадает\n",
      "\n",
      "'эхо москвы со ссылкой на'\t'этом сообщает риа новости со'\tнграмма не совпадает\n",
      "76\t75\tколичество не совпадает\n",
      "\n",
      "'этом сообщает риа новости со'\t'в связи с тем что'\tнграмма не совпадает\n",
      "75\t70\tколичество не совпадает\n",
      "\n",
      "'в связи с тем что'\t'как сообщает итар-тасс со ссылкой'\tнграмма не совпадает\n",
      "70\t58\tколичество не совпадает\n",
      "\n",
      "'по борьбе с организованной преступностью'\t'группировки войск на северном кавказе'\tнграмма не совпадает\n",
      "66\t57\tколичество не совпадает\n",
      "\n",
      "'как сообщает итар-тасс со ссылкой'\t'по борьбе с организованной преступностью'\tнграмма не совпадает\n",
      "58\t55\tколичество не совпадает\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramm_verdict = (\"нграмма не совпадает\", \"нграмма совпадает\")\n",
    "count_verdict  = (\"количество не совпадает\", \"количество совпадает\")\n",
    "\n",
    "for elem1, elem2 in zip(result1, result2):\n",
    "    ngramm1, count1 = elem1\n",
    "    ngramm2, count2 = elem2\n",
    "\n",
    "    print(repr(ngramm1), repr(ngramm2), ngramm_verdict[ngramm1 == ngramm2], sep=\"\\t\")\n",
    "    print(count1, count2, count_verdict[count1 == count2], sep=\"\\t\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583abd1-d046-49a9-9472-3d0826b873ae",
   "metadata": {},
   "source": [
    "<h1>Результат</h1>\n",
    "По большей части пайплайны обработки работают одинаково, однако второй вариант (word_tokenize напрямую) иногда теряет часть валидных совпадений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5781f34",
   "metadata": {},
   "source": [
    "## Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292716e",
   "metadata": {},
   "source": [
    "Найдите какую-то инетересную (по вашему мнению) закономерность на https://books.google.com/ngrams/ для русского языка (с 1990 по 2022)\n",
    "\n",
    "Вставьте сюда скриншот"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4a51a-cece-4851-9b81-335f55f0a317",
   "metadata": {},
   "source": [
    "<img src=\"data/1.png\" align=\"center\">\n",
    "<img src=\"data/2.png\" align=\"center\"> \n",
    "<img src=\"data/3.png\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a89ec",
   "metadata": {},
   "source": [
    "## Заданиe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c35e9",
   "metadata": {},
   "source": [
    "Когда мы разбирали PMI мы использовали такую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221f1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = bigram_count/((word_count_a+word_count_b))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd2def",
   "metadata": {},
   "source": [
    "Но если вы посмотрите на определение в википедии, то увидите, что формула немного другая ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/094243d23c19d2d032f6bb26c4dc4f47d98d32f8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1905862",
   "metadata": {},
   "source": [
    "Перепишите функцию, чтобы она точно соответствовала этому определению. Расчитайте PMI для всех биграммов также как мы делали в семинаре с помощью функции score_bigrams используя изначальный scorer и обновленный. Посмотрите есть ли разница в топ-10 биграммов. Подумайте почему результаты совпадают/отличаются?\n",
    "\n",
    "*Подсказка: для вероятностей можно поделить на количество слов в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1431f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Подгрузка сущностей с семинара\n",
    "\n",
    "sentences = sent_tokenize(data, language='russian')\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "tokenized_sentences = [[token.lower() for token in sentence if not re.match(r'\\W+', token)] \n",
    "                       for sentence in tokenized_sentences]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "token_counts = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    token_counts.update([token for token in sentence if token not in russian_stopwords])\n",
    "\n",
    "bigram_counts = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    bigram_counts.update(ngrammer([token for token in sentence if token not in russian_stopwords]))\n",
    "\n",
    "def ngrammer(tokens, n=2, stops=set()):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def collect_stats(corpus, stops):\n",
    "    ## соберем статистики для отдельных слов\n",
    "    ## и биграммов\n",
    "    \n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    \n",
    "    for sent in corpus:\n",
    "        unigrams.update(sent)\n",
    "        bigrams.update(ngrammer(sent, 2, stops))\n",
    "    \n",
    "    return unigrams, bigrams\n",
    "\n",
    "\n",
    "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000):\n",
    "    ## посчитаем метрику для каждого нграмма\n",
    "    bigram2score = Counter()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        word_a, word_b = bigram.split()\n",
    "        score = scorer(unigrams[word_a], unigrams[word_b], \n",
    "                       bigrams[bigram])\n",
    "        \n",
    "        ## если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdaf8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = collect_stats(tokenized_sentences, russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d413c8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сопоцкина друскеник', 0.5),\n",
       " ('неприятель приблизившись', 0.5),\n",
       " ('саноку обстреливалась', 0.5),\n",
       " ('м.ю лермонтова', 0.5),\n",
       " ('австрийский аэроплан', 0.5),\n",
       " ('показывался аэроплан-птица', 0.5),\n",
       " ('das ist', 0.5),\n",
       " ('ist nesteroff', 0.5),\n",
       " ('песнь нестерове', 0.5),\n",
       " ('могучий унесся', 0.5),\n",
       " ('шумели лязгали', 0.5),\n",
       " ('зловеще гремели.и', 0.5),\n",
       " ('гремели.и пламенно', 0.5),\n",
       " ('жаждали битвы…величие', 0.5),\n",
       " ('равнине обманчиво-зыбкой.презрение', 0.5)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  результат работы скорера с семинара\n",
    "\n",
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)\n",
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee3b66ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('риа новости', 0.4900746163592848),\n",
       " ('северном кавказе', 0.44553483807654565),\n",
       " ('associated press', 0.4345991561181435),\n",
       " ('new york', 0.4218009478672986),\n",
       " ('сих пор', 0.39092055485498106),\n",
       " ('взрывное устройство', 0.3665768194070081),\n",
       " ('таким образом', 0.3657187993680885),\n",
       " ('рао еэс', 0.33954451345755693),\n",
       " ('доменных имен', 0.31512605042016806),\n",
       " ('чрезвычайным ситуациям', 0.30935251798561153),\n",
       " ('налогам сборам', 0.30201342281879195),\n",
       " ('wall street', 0.3018867924528302),\n",
       " ('населенного пункта', 0.3013698630136986),\n",
       " ('объединенной группировки', 0.2993421052631579),\n",
       " ('возбуждено уголовное', 0.2983606557377049)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  результат работы скорера+эвристики с семинара\n",
    "\n",
    "\n",
    "def scorer(word_count_a, word_count_b, bigram_count, min_count=0):\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) / ((word_count_a + word_count_b)))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score\n",
    "\n",
    "# добавим параметр min_count\n",
    "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=0):\n",
    "    ## посчитаем метрику для каждого нграмма\n",
    "    bigram2score = Counter()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        word_a, word_b = bigram.split()\n",
    "        score = scorer(unigrams[word_a], unigrams[word_b], bigrams[bigram], min_count)\n",
    "        \n",
    "        ## если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score\n",
    "\n",
    "bigram2score = score_bigrams(unigrams, bigrams, scorer, min_count=20)\n",
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4a77b",
   "metadata": {},
   "source": [
    "**РЕШЕНИЕ ЗАДАЧИ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bea4905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\AppData\\Local\\Temp\\ipykernel_8736\\3595928878.py:15: RuntimeWarning: invalid value encountered in log2\n",
      "  score = np.log2(score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Solution:\n",
    "\n",
    "    @staticmethod\n",
    "    def scorer(word_a_proba, word_b_proba, bigram_proba, min_count=0, *args):\n",
    "\n",
    "        #  P(a|b) = P(a * b) / P(b)\n",
    "\n",
    "        conditional_proba = (bigram_proba - min_count) / word_b_proba\n",
    "\n",
    "        try:\n",
    "            #  P(a|b) / P(a)\n",
    "            score = conditional_proba / word_a_proba\n",
    "            score = np.log2(score)\n",
    "        except ZeroDivisionError:\n",
    "            return 0\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=0):\n",
    "        ## посчитаем метрику для каждого нграмма\n",
    "        bigram2score = Counter()\n",
    "        uni_corpus_capacity = unigrams.total()\n",
    "        bi_corpus_capacity  = bigrams .total()\n",
    "        \n",
    "        for bigram in bigrams:\n",
    "            word_a, word_b = bigram.split()\n",
    "\n",
    "            score = scorer(\n",
    "                unigrams[word_a] / uni_corpus_capacity,\n",
    "                unigrams[word_b] / uni_corpus_capacity,\n",
    "                (bigrams[bigram] )  / bi_corpus_capacity,\n",
    "                min_count\n",
    "            )\n",
    "            \n",
    "            ## если метрика выше порога, добавляем в словарик\n",
    "            if score > threshold:\n",
    "                bigram2score[bigram] = score\n",
    "        \n",
    "        return bigram2score\n",
    "    \n",
    "bigram2score = Solution.score_bigrams(unigrams, bigrams, Solution.scorer, min_count=20)\n",
    "bigram2score.most_common(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbf807",
   "metadata": {},
   "source": [
    "## Задание 4*\n",
    "\n",
    "Обновите функцию получившуюся в предыдущем задании так, чтобы вместо произведения/деления вероятностей использовались сложение и вычитание логирифмов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "083bdf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\AppData\\Local\\Temp\\ipykernel_8736\\4184021003.py:9: RuntimeWarning: invalid value encountered in log\n",
      "  conditional_proba = np.log(bigram_proba - min_count) - np.log(word_b_proba)\n",
      "C:\\Users\\Kirill\\AppData\\Local\\Temp\\ipykernel_8736\\4184021003.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  conditional_proba = np.log(bigram_proba - min_count) - np.log(word_b_proba)\n",
      "C:\\Users\\Kirill\\AppData\\Local\\Temp\\ipykernel_8736\\4184021003.py:14: RuntimeWarning: divide by zero encountered in log2\n",
      "  score = np.log2(np.exp(score))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wall street', 14.560801346515765),\n",
       " ('саудовской аравии', 14.54895207102774),\n",
       " ('street journal', 14.37671828207679),\n",
       " ('dow jones', 14.350792649635148),\n",
       " ('подписных листов', 14.30417486066219),\n",
       " ('следственном изоляторе', 14.281062505569599),\n",
       " ('чрезвычайным ситуациям', 14.205183360767396),\n",
       " ('france presse', 14.186053568262636),\n",
       " ('персидском заливе', 14.180321069273289),\n",
       " ('полевые командиры', 14.179627299103203),\n",
       " ('полевых командиров', 14.080668819015031),\n",
       " ('налогам сборам', 14.077529111484466),\n",
       " ('следственный изолятор', 14.016128566820326),\n",
       " ('великой отечественной', 14.005180930555012),\n",
       " ('exit polls', 13.993760753791868)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Solution:\n",
    "\n",
    "    @staticmethod\n",
    "    def scorer(word_a_proba, word_b_proba, bigram_proba, min_count=0, *args):\n",
    "\n",
    "        #  P(a|b) = P(a * b) / P(b)\n",
    "        conditional_proba = np.log(bigram_proba - min_count) - np.log(word_b_proba)\n",
    "\n",
    "        try:\n",
    "            #  P(a|b) / P(a)\n",
    "            score = conditional_proba - np.log(word_a_proba)\n",
    "            score = np.log2(np.exp(score))\n",
    "        except ZeroDivisionError:\n",
    "            return 0\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=0):\n",
    "        ## посчитаем метрику для каждого нграмма\n",
    "        bigram2score = Counter()\n",
    "        uni_corpus_capacity = unigrams.total()\n",
    "        bi_corpus_capacity  = bigrams .total()\n",
    "        \n",
    "        for bigram in bigrams:\n",
    "            word_a, word_b = bigram.split()\n",
    "\n",
    "            score = scorer(\n",
    "                unigrams[word_a] / uni_corpus_capacity,\n",
    "                unigrams[word_b] / uni_corpus_capacity,\n",
    "                (bigrams[bigram] - min_count)  / bi_corpus_capacity,\n",
    "            )\n",
    "            \n",
    "            ## если метрика выше порога, добавляем в словарик\n",
    "            if score > threshold:\n",
    "                bigram2score[bigram] = score\n",
    "        \n",
    "        return bigram2score\n",
    "    \n",
    "bigram2score = Solution.score_bigrams(unigrams, bigrams, Solution.scorer, min_count=20)\n",
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22785f4",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1121e53",
   "metadata": {},
   "source": [
    "Исследуйте gensim.models.Phrases. Проверьте сколько дефолтных scoring функций есть в этом классе. Попробуйте все доступные по умолчанию scoring функции и попробуйте настраивать для них значение threshold и min_count. Попробуйте сделать так, чтобы собиралось как можно больше нграммов. Попробуйте строить последовательность gensim.models.Phrases, чтобы строить более длинные нграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64ca81",
   "metadata": {},
   "source": [
    "###  Ход решения:\n",
    "\n",
    "Импортируем библиотеку и посмотрим доку для `gensim.models.phrases.Phrases` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716fba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62ff388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Phrases in module gensim.models.phrases:\n",
      "\n",
      "class Phrases(_PhrasesTransformation)\n",
      " |  Phrases(sentences=None, min_count=5, threshold=10.0, max_vocab_size=40000000, delimiter='_', progress_per=10000, scoring='default', connector_words=frozenset())\n",
      " |\n",
      " |  Detect phrases based on collocation counts.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Phrases\n",
      " |      _PhrasesTransformation\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, sentences=None, min_count=5, threshold=10.0, max_vocab_size=40000000, delimiter='_', progress_per=10000, scoring='default', connector_words=frozenset())\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          The `sentences` iterable can be simply a list, but for larger corpora, consider a generator that streams\n",
      " |          the sentences directly from disk/network, See :class:`~gensim.models.word2vec.BrownCorpus`,\n",
      " |          :class:`~gensim.models.word2vec.Text8Corpus` or :class:`~gensim.models.word2vec.LineSentence`\n",
      " |          for such examples.\n",
      " |      min_count : float, optional\n",
      " |          Ignore all words and bigrams with total collected count lower than this value.\n",
      " |      threshold : float, optional\n",
      " |          Represent a score threshold for forming the phrases (higher means fewer phrases).\n",
      " |          A phrase of words `a` followed by `b` is accepted if the score of the phrase is greater than threshold.\n",
      " |          Heavily depends on concrete scoring-function, see the `scoring` parameter.\n",
      " |      max_vocab_size : int, optional\n",
      " |          Maximum size (number of tokens) of the vocabulary. Used to control pruning of less common words,\n",
      " |          to keep memory under control. The default of 40M needs about 3.6GB of RAM. Increase/decrease\n",
      " |          `max_vocab_size` depending on how much available memory you have.\n",
      " |      delimiter : str, optional\n",
      " |          Glue character used to join collocation tokens.\n",
      " |      scoring : {'default', 'npmi', function}, optional\n",
      " |          Specify how potential phrases are scored. `scoring` can be set with either a string that refers to a\n",
      " |          built-in scoring function, or with a function with the expected parameter names.\n",
      " |          Two built-in scoring functions are available by setting `scoring` to a string:\n",
      " |\n",
      " |          #. \"default\" - :func:`~gensim.models.phrases.original_scorer`.\n",
      " |          #. \"npmi\" - :func:`~gensim.models.phrases.npmi_scorer`.\n",
      " |      connector_words : set of str, optional\n",
      " |          Set of words that may be included within a phrase, without affecting its scoring.\n",
      " |          No phrase can start nor end with a connector word; a phrase may contain any number of\n",
      " |          connector words in the middle.\n",
      " |\n",
      " |          **If your texts are in English, set** ``connector_words=phrases.ENGLISH_CONNECTOR_WORDS``.\n",
      " |\n",
      " |          This will cause phrases to include common English articles, prepositions and\n",
      " |          conjuctions, such as `bank_of_america` or `eye_of_the_beholder`.\n",
      " |\n",
      " |          For other languages or specific applications domains, use custom ``connector_words``\n",
      " |          that make sense there: ``connector_words=frozenset(\"der die das\".split())`` etc.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models.word2vec import Text8Corpus\n",
      " |          >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
      " |          >>>\n",
      " |          >>> # Load corpus and train a model.\n",
      " |          >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |          >>> phrases = Phrases(sentences, min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
      " |          >>>\n",
      " |          >>> # Use the model to detect phrases in a new sentence.\n",
      " |          >>> sent = [u'trees', u'graph', u'minors']\n",
      " |          >>> print(phrases[sent])\n",
      " |          [u'trees_graph', u'minors']\n",
      " |          >>>\n",
      " |          >>> # Or transform multiple sentences at once.\n",
      " |          >>> sents = [[u'trees', u'graph', u'minors'], [u'graph', u'minors']]\n",
      " |          >>> for phrase in phrases[sents]:\n",
      " |          ...     print(phrase)\n",
      " |          [u'trees_graph', u'minors']\n",
      " |          [u'graph_minors']\n",
      " |          >>>\n",
      " |          >>> # Export a FrozenPhrases object that is more efficient but doesn't allow any more training.\n",
      " |          >>> frozen_phrases = phrases.freeze()\n",
      " |          >>> print(frozen_phrases[sent])\n",
      " |          [u'trees_graph', u'minors']\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |\n",
      " |      The ``scoring=\"npmi\"`` is more robust when dealing with common words that form part of common bigrams, and\n",
      " |      ranges from -1 to 1, but is slower to calculate than the default ``scoring=\"default\"``.\n",
      " |      The default is the PMI-like scoring as described in `Mikolov, et. al: \"Distributed\n",
      " |      Representations of Words and Phrases and their Compositionality\" <https://arxiv.org/abs/1310.4546>`_.\n",
      " |\n",
      " |      To use your own custom ``scoring`` function, pass in a function with the following signature:\n",
      " |\n",
      " |      * ``worda_count`` - number of corpus occurrences in `sentences` of the first token in the bigram being scored\n",
      " |      * ``wordb_count`` - number of corpus occurrences in `sentences` of the second token in the bigram being scored\n",
      " |      * ``bigram_count`` - number of occurrences in `sentences` of the whole bigram\n",
      " |      * ``len_vocab`` - the number of unique tokens in `sentences`\n",
      " |      * ``min_count`` - the `min_count` setting of the Phrases class\n",
      " |      * ``corpus_word_count`` - the total number of tokens (non-unique) in `sentences`\n",
      " |\n",
      " |      The scoring function must accept all these parameters, even if it doesn't use them in its scoring.\n",
      " |\n",
      " |      The scoring function **must be pickleable**.\n",
      " |\n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |\n",
      " |  add_vocab(self, sentences)\n",
      " |      Update model parameters with new `sentences`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Text corpus to update this model's parameters from.\n",
      " |\n",
      " |      Example\n",
      " |      -------\n",
      " |      .. sourcecode:: pycon\n",
      " |\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models.word2vec import Text8Corpus\n",
      " |          >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
      " |          >>>\n",
      " |          >>> # Train a phrase detector from a text corpus.\n",
      " |          >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |          >>> phrases = Phrases(sentences, connector_words=ENGLISH_CONNECTOR_WORDS)  # train model\n",
      " |          >>> assert len(phrases.vocab) == 37\n",
      " |          >>>\n",
      " |          >>> more_sentences = [\n",
      " |          ...     [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there'],\n",
      " |          ...     [u'machine', u'learning', u'can', u'be', u'new', u'york', u'sometimes'],\n",
      " |          ... ]\n",
      " |          >>>\n",
      " |          >>> phrases.add_vocab(more_sentences)  # add new sentences to model\n",
      " |          >>> assert len(phrases.vocab) == 60\n",
      " |\n",
      " |  export_phrases(self)\n",
      " |      Extract all found phrases.\n",
      " |\n",
      " |      Returns\n",
      " |      ------\n",
      " |      dict(str, float)\n",
      " |          Mapping between phrases and their scores.\n",
      " |\n",
      " |  freeze(self)\n",
      " |      Return an object that contains the bare minimum of information while still allowing\n",
      " |      phrase detection. See :class:`~gensim.models.phrases.FrozenPhrases`.\n",
      " |\n",
      " |      Use this \"frozen model\" to dramatically reduce RAM footprint if you don't plan to\n",
      " |      make any further changes to your `Phrases` model.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.phrases.FrozenPhrases`\n",
      " |          Exported object that's smaller, faster, but doesn't support model updates.\n",
      " |\n",
      " |  score_candidate(self, word_a, word_b, in_between)\n",
      " |      Score a single phrase candidate.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      (str, float)\n",
      " |          2-tuple of ``(delimiter-joined phrase, phrase score)`` for a phrase,\n",
      " |          or ``(None, None)`` if not a phrase.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _PhrasesTransformation:\n",
      " |\n",
      " |  __getitem__(self, sentence)\n",
      " |      Convert the input sequence of tokens ``sentence`` into a sequence of tokens where adjacent\n",
      " |              tokens are replaced by a single token if they form a bigram collocation.\n",
      " |\n",
      " |              If `sentence` is an entire corpus (iterable of sentences rather than a single\n",
      " |              sentence), return an iterable that converts each of the corpus' sentences\n",
      " |              into phrases on the fly, one after another.\n",
      " |\n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              sentence : {list of str, iterable of list of str}\n",
      " |                  Input sentence or a stream of sentences.\n",
      " |\n",
      " |              Return\n",
      " |              ------\n",
      " |              {list of str, iterable of list of str}\n",
      " |                  Sentence with phrase tokens joined by ``self.delimiter``, if input was a single sentence.\n",
      " |                  A generator of such sentences if input was a corpus.\n",
      " |\n",
      " |      s\n",
      " |\n",
      " |  analyze_sentence(self, sentence)\n",
      " |      Analyze a sentence, concatenating any detected phrases into a single token.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : iterable of str\n",
      " |          Token sequence representing the sentence to be analyzed.\n",
      " |\n",
      " |      Yields\n",
      " |      ------\n",
      " |      (str, {float, None})\n",
      " |          Iterate through the input sentence tokens and yield 2-tuples of:\n",
      " |          - ``(concatenated_phrase_tokens, score)`` for token sequences that form a phrase.\n",
      " |          - ``(word, None)`` if the token is not a part of a phrase.\n",
      " |\n",
      " |  find_phrases(self, sentences)\n",
      " |      Get all unique phrases (multi-word expressions) that appear in ``sentences``, and their scores.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Text corpus.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict(str, float)\n",
      " |         Unique phrases found in ``sentences``, mapped to their scores.\n",
      " |\n",
      " |      Example\n",
      " |      -------\n",
      " |      .. sourcecode:: pycon\n",
      " |\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models.word2vec import Text8Corpus\n",
      " |          >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
      " |          >>>\n",
      " |          >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |          >>> phrases = Phrases(sentences, min_count=1, threshold=0.1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
      " |          >>>\n",
      " |          >>> for phrase, score in phrases.find_phrases(sentences).items():\n",
      " |          ...     print(phrase, score)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from _PhrasesTransformation:\n",
      " |\n",
      " |  load(*args, **kwargs)\n",
      " |      Load a previously saved :class:`~gensim.models.phrases.Phrases` /\n",
      " |      :class:`~gensim.models.phrases.FrozenPhrases` model.\n",
      " |\n",
      " |      Handles backwards compatibility from older versions which did not support pluggable scoring functions.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : object\n",
      " |          See :class:`~gensim.utils.SaveLoad.load`.\n",
      " |      kwargs : object\n",
      " |          See :class:`~gensim.utils.SaveLoad.load`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |\n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |\n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |\n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |\n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |\n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |\n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |\n",
      " |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      " |      Save the object to a file.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      " |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      " |          loading and sharing the large arrays in RAM between multiple processes.\n",
      " |\n",
      " |          If list of str: store these attributes into separate files. The automated size check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int, optional\n",
      " |          Don't store arrays smaller than this separately. In bytes.\n",
      " |      ignore : frozenset of str, optional\n",
      " |          Attributes that shouldn't be stored at all.\n",
      " |      pickle_protocol : int, optional\n",
      " |          Protocol number for pickle.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |          Load object from file.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8258a",
   "metadata": {},
   "source": [
    "Видим, что есть 2 дефолтные скоринг-функции: `scoring : {'default', 'npmi', function}, optional`. Их и будем проверять.\n",
    "\n",
    "Для начала посмотрим как поведет себя параметр `\"default\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b979e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wall street', 9993.185185185186),\n",
       " ('саудовской аравии', 9911.444378346223),\n",
       " ('street journal', 8796.08487654321),\n",
       " ('dow jones', 8639.42857142857),\n",
       " ('подписных листов', 8364.72496025437),\n",
       " ('следственном изоляторе', 8231.787549407114),\n",
       " ('чрезвычайным ситуациям', 7810.021955260977),\n",
       " ('france presse', 7707.146484375),\n",
       " ('персидском заливе', 7676.583164983164),\n",
       " ('полевые командиры', 7672.892500000001),\n",
       " ('полевых командиров', 7164.232026143792),\n",
       " ('следственный изолятор', 6850.796875),\n",
       " ('великой отечественной', 6799.007559604574),\n",
       " ('exit polls', 6745.400000000001),\n",
       " ('верховной рады', 6643.19696969697)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_scoring_phrases = Phrases(tokenized_sentences, min_count=20, scoring=\"default\", delimiter=\" \")\n",
    "\n",
    "phrase2score = Counter(default_scoring_phrases.export_phrases())\n",
    "\n",
    "phrase2score.most_common(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c176f7c",
   "metadata": {},
   "source": [
    "Результаты получились почти такие же, как у нас в заданиях 3 и 4.\n",
    "\n",
    "Теперь посмотрим на то, как себя поведет параметр `\"npmi\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d59c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4536\n",
      "[('dow jones', 0.9967760691772763),\n",
      " ('exit polls', 0.9964338214125783),\n",
      " ('wall street', 0.9963236116179338),\n",
      " ('риа новости', 0.995285953928321),\n",
      " ('норильский никель', 0.9926038996026336),\n",
      " ('саудовской аравии', 0.9905222989112752),\n",
      " ('new york', 0.9861815558365328),\n",
      " ('подписные листы', 0.9860355438566462),\n",
      " ('ak m', 0.986004496354122),\n",
      " ('северном кавказе', 0.9820691855789901),\n",
      " ('associated press', 0.9818890021904133),\n",
      " ('подписных листов', 0.9808571675497766),\n",
      " ('чрезвычайным ситуациям', 0.9805515372859319),\n",
      " ('street journal', 0.9804680075917059),\n",
      " ('взрывном устройстве', 0.9775056185674951)]\n"
     ]
    }
   ],
   "source": [
    "default_scoring_phrases = Phrases(tokenized_sentences, min_count=20, scoring=\"npmi\", delimiter=\" \", threshold=0.1)\n",
    "\n",
    "phrase2score = Counter(default_scoring_phrases.export_phrases())\n",
    "\n",
    "print(len(phrase2score))\n",
    "pprint(phrase2score.most_common(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2565ebc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "[('риа новости', 0.995285953928321),\n",
      " ('северном кавказе', 0.9820691855789901),\n",
      " ('associated press', 0.9818890021904133),\n",
      " ('сих пор', 0.9592748163641645),\n",
      " ('таким образом', 0.9294529760197355),\n",
      " ('возбуждено уголовное', 0.9103101486527193),\n",
      " ('объединенной группировки', 0.8979649705683589),\n",
      " ('со ссылкой', 0.8905919454977269),\n",
      " ('уголовное дело', 0.8877541186013141),\n",
      " ('правоохранительных органов', 0.8816298343877539),\n",
      " ('кроме того', 0.8760857667614927),\n",
      " ('внутренних дел', 0.8750484508025371),\n",
      " ('эхо москвы', 0.8614347676014397),\n",
      " ('федеральных сил', 0.8475704878071869),\n",
      " ('московскому времени', 0.8452137416311057)]\n"
     ]
    }
   ],
   "source": [
    "npmi_scoring_phrases = Phrases(tokenized_sentences, min_count=200, scoring=\"npmi\", delimiter=\" \", threshold=0.7)\n",
    "\n",
    "phrase2score = Counter(npmi_scoring_phrases.export_phrases())\n",
    "\n",
    "print(len(phrase2score))\n",
    "pprint(phrase2score.most_common(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66085cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5043\n",
      "[('dow jones', 0.9967760691772763),\n",
      " ('exit polls', 0.9964338214125783),\n",
      " ('wall street', 0.9963236116179338),\n",
      " ('риа новости', 0.995285953928321),\n",
      " ('норильский никель', 0.9926038996026336),\n",
      " ('саудовской аравии', 0.9905222989112752),\n",
      " ('new york', 0.9861815558365328),\n",
      " ('подписные листы', 0.9860355438566462),\n",
      " ('ak m', 0.986004496354122),\n",
      " ('северном кавказе', 0.9820691855789901),\n",
      " ('associated press', 0.9818890021904133),\n",
      " ('подписных листов', 0.9808571675497766),\n",
      " ('чрезвычайным ситуациям', 0.9805515372859319),\n",
      " ('street journal', 0.9804680075917059),\n",
      " ('взрывном устройстве', 0.9775056185674951)]\n"
     ]
    }
   ],
   "source": [
    "npmi_scoring_phrases = Phrases(tokenized_sentences, min_count=20, scoring=\"npmi\", delimiter=\" \", threshold=0)\n",
    "\n",
    "phrase2score = Counter(npmi_scoring_phrases.export_phrases())\n",
    "\n",
    "print(len(phrase2score))\n",
    "pprint(phrase2score.most_common(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee9d565a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2353\n",
      "[('philip morris', 21651.9012345679),\n",
      " ('della sera', 21239.840830449826),\n",
      " ('брюшного тифа', 21239.840830449826),\n",
      " ('exit polls', 20236.2),\n",
      " ('норильский никель', 19642.6048),\n",
      " ('саудовская аравия', 19486.71111111111),\n",
      " ('голден ада', 19486.71111111111),\n",
      " ('dow jones', 19438.714285714286),\n",
      " ('подписные листы', 18268.791666666664),\n",
      " ('ak m', 18043.251028806586),\n",
      " ('polar lander', 17936.631818181817),\n",
      " ('аум синрике', 17715.191919191922),\n",
      " ('corriere della', 17194.156862745098),\n",
      " ('взрывном устройстве', 17012.208112874778),\n",
      " ('духовное наследие', 16576.597353497164)]\n"
     ]
    }
   ],
   "source": [
    "bigram_model = Phrases(tokenized_sentences, min_count=10, scoring=\"default\", delimiter=\" \", threshold=100)\n",
    "\n",
    "phrase2score = Counter(bigram_model.export_phrases())\n",
    "\n",
    "print(len(phrase2score))\n",
    "pprint(phrase2score.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff6f3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2709\n",
      "[('new york', 6986639.458333334),\n",
      " ('саудовской аравии', 6500937.25),\n",
      " ('риа новости со ссылкой', 4427362.4375),\n",
      " ('ak m', 3362553.75),\n",
      " ('риа новости', 2301701.0080882353),\n",
      " ('mars polar', 2092255.6666666667),\n",
      " ('подписных листов', 1972698.2000000002),\n",
      " ('bank of new york', 1522934.3968253967),\n",
      " ('уголовного розыска', 1382383.2083333335),\n",
      " ('эхо москвы со ссылкой', 1315132.1333333333),\n",
      " ('всея руси', 1195574.6666666665),\n",
      " ('чрезвычайным ситуациям', 1188102.325),\n",
      " ('associated press', 1135084.2817460317),\n",
      " ('самодельное взрывное', 1120851.25),\n",
      " ('полевые командиры', 1064808.6875)]\n"
     ]
    }
   ],
   "source": [
    "trigram_model = Phrases(bigram_model[tokenized_sentences], min_count=10, scoring=\"default\", delimiter=\" \", threshold=100)\n",
    "\n",
    "phrase2score = Counter(trigram_model.export_phrases())\n",
    "\n",
    "print(len(phrase2score))\n",
    "pprint(phrase2score.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f2a9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "npmi_bigram_model = Phrases(tokenized_sentences, min_count=20, scoring=\"npmi\", delimiter=\" \", threshold=0)\n",
    "npmi_trigram_model = Phrases(npmi_bigram_model[tokenized_sentences], min_count=20, scoring=\"npmi\", delimiter=\" \", threshold=0)\n",
    "npmi_quadram_model = Phrases(npmi_trigram_model[tokenized_sentences], min_count=20, scoring=\"npmi\", delimiter=\" \", threshold=0)\n",
    "\n",
    "phrase2score = Counter(npmi_quadram_model.export_phrases())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94aea007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4432\n",
      "('риа новости со ссылкой', 1.8094118420833298)\n",
      "('эхо москвы со ссылкой', 1.4643552158004989)\n",
      "('контртеррористической операции на северном', 1.1601283021285917)\n",
      "('сообщает интерфакс со ссылкой', 1.1353505165376638)\n",
      "('сообщает итар-тасс со ссылкой', 1.1318084948444513)\n",
      "('риа новости в пресс-службе', 1.1051204158018615)\n",
      "('сообщает рбк со ссылкой', 1.0484580188430923)\n",
      "('об этом сообщает риа', 1.0064785481882763)\n",
      "('минимального размера оплаты труда', 0.9878491273596968)\n"
     ]
    }
   ],
   "source": [
    "print(len(phrase2score))\n",
    "for phrase in phrase2score.most_common(300):\n",
    "    if len(phrase[0].split()) >= 4:\n",
    "        print(phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
