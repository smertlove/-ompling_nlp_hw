{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48abae58",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "\n",
    "Выберите 5 языков в википедии (не тех, что использовались в семинаре). Скачайте по 10 случайных статей для каждого языка. Предобработайте тексты, удаляя лишние теги/отступы/разделители (если они есть). Разделите тексты на предложения и создайте датасет, в котором каждому предложению соответствует язык. Кластеризуйте тексты, используя эбмединг модель из прошлого семинара и любой алгоритм кластеризации. Проверьте качество кластеризации с помощь метрики ARI. Отдельно проанализируйте 3 ошибочно кластеризованных текста (если такие есть)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1769f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f52a5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_page():\n",
    "    try:\n",
    "        return wikipedia.random(1)\n",
    "    except Exception:\n",
    "        return get_random_page()\n",
    "    \n",
    "def get_page_by_name(name:str):\n",
    "    try:\n",
    "        return wikipedia.page(name)\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        random_option = np.random.choice(e.options)\n",
    "        return get_page_by_name(random_option)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b506e28",
   "metadata": {},
   "source": [
    "####  Код ниже можнно не запускать, под ним ячейка со спиклованным объектом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d18c8826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSE subj data\\актуальные проблемы компьютерной лингвистики\\.venv\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file d:\\HSE subj data\\актуальные проблемы компьютерной лингвистики\\.venv\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "lang_to_pages = {\n",
    "    \"zh\": dict(),  #  китайский\n",
    "    \"ja\": dict(),  #  японский\n",
    "    \"ko\": dict(),  #  корейский\n",
    "    \"hi\": dict(),  #  хинди\n",
    "    \"lv\": dict(),  #  латышский\n",
    "}\n",
    "for lang in lang_to_pages:\n",
    "    wikipedia.set_lang(lang)\n",
    "    for _ in range(10):\n",
    "        random_page = get_random_page()\n",
    "        lang_to_pages[lang][random_page] = get_page_by_name(random_page).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303510cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a38d0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data/lang_to_pages.pkl\", \"wb\") as file:\n",
    "    pickle.dump(lang_to_pages, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d32714",
   "metadata": {},
   "source": [
    "## Загрузка объекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eafbcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r\"data/lang_to_pages_COPY.pkl\", \"rb\") as file:\n",
    "    lang_to_pages = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed71172",
   "metadata": {},
   "source": [
    "### Чистка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0187f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class NormRule:\n",
    "    def __init__(self, from_:str, to_:str) -> None:\n",
    "        self.from_ = from_\n",
    "        self.to_ = to_\n",
    "\n",
    "    def apply_rule(self, text:str) -> str:\n",
    "        return text.replace(self.from_, self.to_)\n",
    "\n",
    "\n",
    "class RegNormRule(NormRule):\n",
    "\n",
    "    def apply_rule(self, text: str) -> str:\n",
    "        return re.sub(self.from_, self.to_, text)\n",
    "\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "\n",
    "    rules = (\n",
    "        NormRule(\"==\", \"\"),\n",
    "        RegNormRule(r\"\\s+\", \" \"),\n",
    "        RegNormRule(\"・+\", \" \"),\n",
    "    )\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def normalize(cls, text:str) -> str:\n",
    "        for rule in cls.rules:\n",
    "            text = rule.apply_rule(text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3fdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in lang_to_pages:\n",
    "    lang_to_title = lang_to_pages[lang]\n",
    "    for title in lang_to_title:\n",
    "        lang_to_title[title] = Normalizer.normalize(lang_to_title[title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c072ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/smertlove/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abee0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c234c28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'HED.Cycling Products(日:ヘッド サイクリング プロダクト)は、ミネソタ州セントポールの自転車店Grand Performanceオーナーであったスティーブ ヘッド(英:Steve Hed)により、「トライアスリートの為に現代的で手頃な価格のエアロ ディスクホイールを作る」というコンセプトの元、1984年に誕生した自転車部品メーカーである。 ロードレースの世界ではあまり目立たないメーカーだが、トライアスリートやメッセンジャーなどシングルスピードバイクのユーザーからは絶大な人気のあるホイールブランドである。 歴史 1984年：ミネソタ州セントポールの自転車店オーナーであったスティーブ ヘッドにより創業 1989年：当時、模型飛行機の製作に時間を費やし、飛行のあらゆる側面を研究していたスティーブは、従来の自転車のホイールはV字型のくさび形をしていて、空気力学上で、アドバンテージが得られていない事を発見し、リム断面がトロイダル型のHED CXを開発、60mmハイト、26mm幅という画期的なリムを設計 1988年：スティーブはトライアスロンの選手に空気力学的な優位性を与えるために、ウォーターボトルの収納場所をライダーの後ろ、そしてライダーが生み出す自然なスリップストリームの内側に再配置することを思いつきました。この新製品を「Tails」と名付け、シート後方用ボトルホルダーを開発。 1992年：スティーブ達は、初のカーボンファイバー製トロイダル型ホイールを発表しました。「ジェットスタイル ホイール」と名付け、業界をリードするジェットシリーズの初製品となりました。 1995年：アメリカのマウンテンバイクブームが起こると、HEDはエアロダイナミックの専門知識をゲレンデやパークに持ち込こみ、エアロダイナミックのダウンヒルMTBとBMXホイールを発表。 1998年：HED社はアメリカのデュポン社と協力して、従来のスポークの代わりに3本の空力フォイルを備えたカーボンホイールという、型破りなホイールデザインを獲得しました、3本のスポークは丸みを帯びたリーディングエッジとシャープなトレーリングエッジを持ち、これまでにないパフォーマンスの優位性を生み出しました。 2001年：更なる製品開発を求めていたスティーブは、今度はエアロバーに目を向け、１0年以上かけてHEDサイクリングのチームは、ライダーに空力と重量の両方の利点を与えるハンドルバーを作ることに専念しました。その結果、初のカーボンエアロバーが誕生しました。 2007年：エアロディスク市場の原点に立ち返り、Jet Aero Discを発表。 2014年：創業者のスティーブ ヘッドが仕事中に倒れ59歳で急逝。その後、妻のアン ヘッドがCEOに就任、女性が経営する企業として成長を続け、ミネソタ州ローズビルの新しい生産施設に移転。 2015年：ツール ド フランスで、当時BMCレーシングに所属していたローハン デニスが、タイムトライアルにてGT3を使用し、約14kmの間、平均時速55.45kmの猛烈な速さで走り、ステージを1位で終え、2015年のツール ド フランスで初のイエロージャージを獲得。 2016年：HEDとCerveloが共同開発し、P5Xの誕生。 2019年：HED Cycling Products incは創業35周年を迎える 製品の特長 リム幅が内径21mmのスーパーワイドリムになっており、28c等のタイヤを装着してもリム面とタイヤの間に隙間がなくなり空気抵抗が抑えられる様設計されており、太いタイヤまで装着可能になっている。 また、アルミリム採用の数モデルには、「カーボンクリンチャーキラー」というコンセプトの基に開発されたBLACK SERIESがあり。黒いブレーキ面はPEO処理ではなくアルマイトによるもの。加えて切削加工によって溝が設けられており、ドライコンディションでは25％、ウェットコンディションでは70％もの制動力の向上を実現しているという。 製品ラインナップ 2021年現在、HEDではホイールのグレードがPROとPERFORMANCEの2つに分かれており、基本的にはハブやスポークの違いだが、グラベルなどではフルカーボンリムとアルミリムとで分けられているものもある。 合わせてHEDのホイールは前後別売りされており、トライアスロンやロードレースなどの大会において使用環境に合わせホイールの選択ができるような販売方法をしている。 = ロード＋トライアスロン = JET SERIES リムブレーキ JET RC BLACK SERIES(RC4、RC6、RC9) JET RC PERFORMANCE SERIES(RC4、RC6、RC9) JET RC BLACK(PERFORMANCE) Aero Disc VANQUISH SERIES ディスクブレーキ VANQUISH RC PRO SERIES(RC4、RC6、RC8、RDW) VANQUISH RC PERFORMANCE SERIES(RC4、RC6) ARDENNES SERIES リム ディスクブレーキ ARDENNES RA BLACK ARDENNES RA PRO(リムブレーキモデル) ARDENNES RA PRO DISC ARDENNES RA PERFORMANCE DISC = グラベル = EMPORIA SERIES ディスクブレーキ EMPORIA GC3 PRO EMPORIA GC3 PERFORMANCE EMPORIA GA PRO - Silver Edition EMPORIA GA PRO EMPORIA GA PERFORMANCE = ファットバイク = BIG DEAL SERIES ディスクブレーキ BIG HALF DEAL(27.5inch) BIG DEAL(26inch) BIG ALUMINUM DEAL BIG HALF ALUMINUM DEAL = トラック = VOLO Onyxハブ、ホワイトインダストリーのコグで組み合わせ JET TRACK(RC4、RC6、RC9) = 過去に製造していたモデル = GT3(フルカーボンバトンホイール) H3(アルミリムカーボンバトンホイール) STINGER(カーボンチューブラーホイール) RAPTOR(カーボンMTBホイール) JET 17inc(Alex Moulton用) サポートライダー ブラッテン カリー クララ ブラウン Emilio Aguayo Eneko Llanos Ivan Rana Fuentes Jocelyn McCauley ライオネル サンダース Matt Hanson Michael Weiss ロナルド クバ サラ ペレス Sika Henry 外部リンク 公式ウェブサイト HED Cycling Official - YouTubeチャンネル HED Cycling Products (@HEDCycling) - Twitter HED CYCLING PRODUCTS (hedwheels) - Instagram HED Cycling Official - Facebook 参考 '\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/japanese/\u001b[0m\n\n  Searched in:\n    - '/home/smertlove/nltk_data'\n    - '/home/smertlove/sandbox/hse/nlp_hw/venv/nltk_data'\n    - '/home/smertlove/sandbox/hse/nlp_hw/venv/share/nltk_data'\n    - '/home/smertlove/sandbox/hse/nlp_hw/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m txt \u001b[38;5;241m=\u001b[39m lang_to_pages[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mja\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;66;03m#  ko hi lv\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(txt))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjapanese\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/japanese/\u001b[0m\n\n  Searched in:\n    - '/home/smertlove/nltk_data'\n    - '/home/smertlove/sandbox/hse/nlp_hw/venv/nltk_data'\n    - '/home/smertlove/sandbox/hse/nlp_hw/venv/share/nltk_data'\n    - '/home/smertlove/sandbox/hse/nlp_hw/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "key = choice(list(lang_to_pages[\"ja\"].keys()))\n",
    "txt = lang_to_pages[\"ja\"][key] #  ko hi lv\n",
    "\n",
    "print(repr(txt))\n",
    "print(len(sent_tokenize(txt, \"japanese\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b93c55",
   "metadata": {},
   "source": [
    "### NLTK не поддерживает такую экзотику, значит будем городить своё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df86be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    delimiter = \".\"\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text:str) -> list[str]: \n",
    "        return text.split(cls.delimiter)\n",
    "\n",
    "\n",
    "class HieroglyphicTokenizer(Tokenizer):\n",
    "    delimiter = \"。\"\n",
    "\n",
    "\n",
    "class DevanagariTokenizer(Tokenizer):\n",
    "    delimiter = \"।\"\n",
    "\n",
    "\n",
    "class RegularTokenizer(Tokenizer):\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text: str) -> list[str]:\n",
    "        return nltk.sent_tokenize(text)\n",
    "\n",
    "\n",
    "lang_to_tokenizer : dict[str, Tokenizer]= {\n",
    "    \"zh\": HieroglyphicTokenizer,\n",
    "    \"ja\": HieroglyphicTokenizer,\n",
    "    \"ko\": Tokenizer,\n",
    "    \"hi\": DevanagariTokenizer,\n",
    "    \"lv\": RegularTokenizer,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50bc54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i, lang in enumerate(lang_to_pages):\n",
    "    tokenizer = lang_to_tokenizer[lang]\n",
    "    lang_to_title = lang_to_pages[lang]\n",
    "\n",
    "    for title in lang_to_title:\n",
    "        sents = tokenizer.tokenize(lang_to_title[title])\n",
    "        \n",
    "        for sent in sents:\n",
    "            corpus.append((sent.strip(), lang, i + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effe0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a8b8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(corpus, columns=(\"sentence\", \"language\", \"lang_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a165e774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "      <th>lang_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>布拉迪斯拉發施洛雲體育會（斯洛伐克語：ŠK Slovan Bratislava），簡稱施洛雲...</td>\n",
       "      <td>zh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>斯拉夫人是斯洛伐克最為成功的足球俱樂部之一</td>\n",
       "      <td>zh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>在捷克斯洛伐克時代，斯拉夫人是獲得國內聯賽冠軍次數最多的斯洛伐克球隊（共計 8 次）</td>\n",
       "      <td>zh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>球隊還在1969年5月21日以 3–2 击败巴塞罗那，獲得了歐洲盃賽冠軍盃冠军</td>\n",
       "      <td>zh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>斯洛伐克独立后，斯拉夫人成为国内最大豪门，屡次夺得联赛和杯赛冠军</td>\n",
       "      <td>zh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>Pilsēta pirmoreiz minēta rakstos 960. gadā.</td>\n",
       "      <td>lv</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Burgrāfistes centrs, līdz 1460. gadā, tas tika...</td>\n",
       "      <td>lv</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Atsauces Ārējās saites Vikikrātuvē par šo tēmu...</td>\n",
       "      <td>lv</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Skatīt: Dona.</td>\n",
       "      <td>lv</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Brockhaus Enzyklopädie raksts (vāciski)</td>\n",
       "      <td>lv</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>797 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence language  lang_id\n",
       "0    布拉迪斯拉發施洛雲體育會（斯洛伐克語：ŠK Slovan Bratislava），簡稱施洛雲...       zh        1\n",
       "1                                斯拉夫人是斯洛伐克最為成功的足球俱樂部之一       zh        1\n",
       "2           在捷克斯洛伐克時代，斯拉夫人是獲得國內聯賽冠軍次數最多的斯洛伐克球隊（共計 8 次）       zh        1\n",
       "3              球隊還在1969年5月21日以 3–2 击败巴塞罗那，獲得了歐洲盃賽冠軍盃冠军       zh        1\n",
       "4                     斯洛伐克独立后，斯拉夫人成为国内最大豪门，屡次夺得联赛和杯赛冠军       zh        1\n",
       "..                                                 ...      ...      ...\n",
       "792        Pilsēta pirmoreiz minēta rakstos 960. gadā.       lv        5\n",
       "793  Burgrāfistes centrs, līdz 1460. gadā, tas tika...       lv        5\n",
       "794  Atsauces Ārējās saites Vikikrātuvē par šo tēmu...       lv        5\n",
       "795                                      Skatīt: Dona.       lv        5\n",
       "796            Brockhaus Enzyklopädie raksts (vāciski)       lv        5\n",
       "\n",
       "[797 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34436e1d",
   "metadata": {},
   "source": [
    "###  Загружаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0393d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1300c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b165c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d9cfb",
   "metadata": {},
   "source": [
    "###  Получим кластеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ae4a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = df['sentence'].values\n",
    "langs = df[\"language\"].values\n",
    "\n",
    "X = np.zeros((len(sents), 768))\n",
    "\n",
    "for i, sent, lang in zip(range(len(sents)), sents, langs):\n",
    "    X[i] = model.encode(sent)\n",
    "\n",
    "cluster = KMeans(\n",
    "    n_clusters = len(lang_to_pages),\n",
    "    max_iter = 500,\n",
    "    algorithm = 'elkan',\n",
    "    random_state=21  ##  нужно чтобы результат кластеризации всегда был одинаковый\n",
    "    )\n",
    "cluster.fit(X)\n",
    "\n",
    "predicted_labels = np.array(cluster.labels_)+1\n",
    "true_labels = df[\"lang_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c681c",
   "metadata": {},
   "source": [
    "###  Посчитаем ARI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ddca423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5025935809760352\n"
     ]
    }
   ],
   "source": [
    "ARI = adjusted_rand_score(true_labels, predicted_labels)\n",
    "        \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b88230",
   "metadata": {},
   "source": [
    "###  Теперь надо собоставить кластеры языкам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "962205af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 1, 3, 1, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 2, 4, 2, 2, 4, 2, 2,\n",
       "       2, 4, 4, 4, 2, 4, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4,\n",
       "       4, 4, 2, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2,\n",
       "       4, 2, 4, 2, 2, 2, 2, 2, 4, 3, 2, 4, 2, 4, 2, 2, 4, 4, 4, 2, 2, 2,\n",
       "       2, 4, 4, 4, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 4,\n",
       "       4, 2, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2, 4, 2, 2, 4, 2, 2, 2, 4, 2, 4,\n",
       "       4, 2, 4, 2, 4, 2, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4,\n",
       "       2, 2, 2, 4, 1, 4, 2, 2, 2, 2, 1, 1, 2, 4, 2, 4, 3, 4, 4, 4, 2, 4,\n",
       "       2, 2, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 4, 4, 2, 2, 4,\n",
       "       2, 2, 4, 2, 2, 2, 4, 2, 4, 2, 3, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2,\n",
       "       2, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 1, 5, 1, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1,\n",
       "       5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 1, 1, 1, 5, 1, 1, 1, 1,\n",
       "       5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Посмотрим, как распределились кластеры:\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68808f4",
   "metadata": {},
   "source": [
    "### Кластеры явно видны.\n",
    "\n",
    "Это значит, что в целом мы можем сопоставить язык и номер кластера. Сделаем это так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7938196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zh', 231), ('ja', 201), ('ko', 229), ('hi', 45), ('lv', 91)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Посмотрим, сколько у нас предложений на каждый язык:\n",
    "\n",
    "grouped_df = df.groupby(\"language\")\n",
    "\n",
    "langs_and_counts = [\n",
    "        (key, len(grouped_df.get_group(key)))\n",
    "        for key\n",
    "        in (\"zh\", \"ja\", \"ko\", \"hi\", \"lv\")  ##  порядок важен\n",
    "    ]\n",
    "langs_and_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4558e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 'zh', 1: 'ja', 2: 'ko', 5: 'hi', 4: 'lv'}\n"
     ]
    }
   ],
   "source": [
    "#  Посчитаем сымые частнотные предсказания для каждого языка\n",
    "\n",
    "def get_most_common_from_list(target:list, keys:set) -> int:\n",
    "\n",
    "    counts = {obj : target.count(obj) for obj in keys}\n",
    "    return max(counts, key=lambda c: counts[c])\n",
    "\n",
    "\n",
    "cluster_to_lang = dict()\n",
    "\n",
    "clusters = {1,2,3,4,5}\n",
    "\n",
    "offset = 0\n",
    "for lang, count in langs_and_counts:\n",
    "    window = predicted_labels[offset:offset+count]\n",
    "    most_common = get_most_common_from_list(list(window), clusters)\n",
    "\n",
    "    cluster_to_lang[most_common] = lang\n",
    "\n",
    "    clusters.remove(most_common)\n",
    "    offset += count\n",
    "\n",
    "print(cluster_to_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccd80c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    @classmethod\n",
    "    def cast_vector(cls, row):\n",
    "        return np.array(list(map(lambda x: x.astype('double'), row)))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def predict(cls, text:str) -> str:\n",
    "        emb = model.encode(text).reshape(1, -1)\n",
    "        emb = cls.cast_vector(emb)\n",
    "\n",
    "        return cluster_to_lang[int(cluster.predict(emb)[0])+ 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "927ae15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "红红的小脸儿温暖我的心窝\tzh\n",
      "点亮我生命的火 火火火火\tzh\n",
      " 一步一步地会到目的\tzh\n",
      "蝸牛そろそろ登れ富士の山\tzh\n",
      "私の名は吉良吉影。\tzh\n",
      "오빤 강남 스타일, uh 강남 스타일 옵, 옵-옵-옵, 오빤 강남 스타일, uh 강남 스타일 옵, 옵-옵-옵, 오빤 강남 스타일\tko\n",
      "내가 무슨 언어야?\tko\n",
      "잘 지내세요? \tko\n",
      "हिन्दी में टाइप करें\thi\n",
      "खुशी उन लोगों को नहीं आएगी जो नहीं जानते कि उनके पास पहले से क्या सराहना है\thi\n",
      "Jo es gauži bēdājos, Jo nelaime priecājas\thi\n",
      "Kad cer, tad notic tik viegli. 1 Patīk. – Ērihs Marija Remarks. \thi\n"
     ]
    }
   ],
   "source": [
    "sample_texts = (\n",
    "    \"红红的小脸儿温暖我的心窝\",  ##  zh\n",
    "    \"点亮我生命的火 火火火火\",   ##  zh\n",
    "    \"一步一步地会到目的\",          ##  zh\n",
    "    \"蝸牛そろそろ登れ富士の山\",  ##  ja\n",
    "    \"私の名は吉良吉影。\",   ##  ja\n",
    "    \"오빤 강남 스타일, uh 강남 스타일 옵, 옵-옵-옵, 오빤 강남 스타일, uh 강남 스타일 옵, 옵-옵-옵, 오빤 강남 스타일\",  ##  ko\n",
    "    \"내가 무슨 언어야?\",  ##  ko\n",
    "    \"잘 지내세요? \",  ##  ko\n",
    "    \"हिन्दी में टाइप करें\",  ##  hi\n",
    "    \"खुशी उन लोगों को नहीं आएगी जो नहीं जानते कि उनके पास पहले से क्या सराहना है\",  ##  hi\n",
    "    \"Jo es gauži bēdājos, Jo nelaime priecājas\",  ##  lv\n",
    "    \"Kad cer, tad notic tik viegli. 1 Patīk. – Ērihs Marija Remarks. \", ##  lv\n",
    "\n",
    ")\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(text, Predictor.predict(text), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5bcc9",
   "metadata": {},
   "source": [
    "###  Путает пары японский-китайский и хинди-латышский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2847814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e561321",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for i, sent, lang in zip(range(len(sents)), sents, langs):\n",
    "    true_labels.append(lang)\n",
    "    predicted_labels.append(Predictor.predict(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a142bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          hi       0.32      0.84      0.46        45\n",
      "          ja       0.07      0.01      0.02       201\n",
      "          ko       0.99      0.60      0.75       229\n",
      "          lv       0.00      0.00      0.00        91\n",
      "          zh       0.52      0.97      0.68       231\n",
      "\n",
      "    accuracy                           0.50       797\n",
      "   macro avg       0.38      0.48      0.38       797\n",
      "weighted avg       0.47      0.50      0.44       797\n",
      "\n",
      "[[ 38   7   0   0   0]\n",
      " [  0   2   1   0 198]\n",
      " [  0   4 138  82   5]\n",
      " [ 80  11   0   0   0]\n",
      " [  1   6   0   1 223]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))\n",
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd75839",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "Загрузите корпус `annot.opcorpora.no_ambig_strict.xml.bz2` с OpenCorpora. Найдите в корпусе самые частотные морфологически омонимичные словоформы (те, которым соответствует разный грамматический разбор в разных предложениях). Также найдите словоформы с самых большим количеством вариантов грамматических разборов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ed11ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0f959d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(r\"data/annot.opcorpora.no_ambig_strict.xml\", \"rb\") as file:\n",
    "    open_corpora = lxml.etree.fromstring(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ffece37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "\n",
    "for sentence in open_corpora.xpath('//tokens'):\n",
    "    sent_tagged = []\n",
    "    for token in sentence.xpath('token'):\n",
    "        word = token.xpath('@text')\n",
    "        gram_info = token.xpath('tfr/v/l/g/@v')\n",
    "        sent_tagged.append([word[0].lower()] + gram_info)\n",
    "    \n",
    "    corpus.append(sent_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "be265156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['сохранится', 'VERB', 'perf', 'intr', 'sing', '3per', 'futr', 'indc'],\n",
       " ['ли', 'PRCL'],\n",
       " ['градус', 'NOUN', 'inan', 'masc', 'sing', 'nomn'],\n",
       " ['дискуссии', 'NOUN', 'inan', 'femn', 'sing', 'gent'],\n",
       " ['в', 'PREP'],\n",
       " ['новом', 'ADJF', 'Qual', 'masc', 'sing', 'loct'],\n",
       " ['сезоне', 'NOUN', 'inan', 'masc', 'sing', 'loct'],\n",
       " ['?', 'PNCT']]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1e2e117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5ca7db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d:dict[str, set] = dict()\n",
    "\n",
    "entry_tmpl = {\n",
    "    \"count\": 0,\n",
    "    \"tagset_set\": set()\n",
    "}\n",
    "\n",
    "for sentence in corpus:\n",
    "    for token in sentence:\n",
    "        word = token[0]\n",
    "        tagset = ';'.join(token[1:])\n",
    "\n",
    "        entry = d.get(word, deepcopy(entry_tmpl))\n",
    "        entry[\"tagset_set\"].add(tagset)\n",
    "        entry[\"count\"] += 1\n",
    "\n",
    "        d[word] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e756c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ambig = {\n",
    "    key:d[key]\n",
    "    for key\n",
    "    in d\n",
    "    if len(d[key][\"tagset_set\"]) > 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0f00b6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cамые частотные морфологически омонимичные словоформы:\n",
      "слово\tколичество форм\tстатья\n",
      "в\t2059\t{'count': 2059, 'tagset_set': {'PREP', 'NOUN;inan;masc;Fixd;Abbr;sing;gent'}}\n",
      "на\t786\t{'count': 786, 'tagset_set': {'PREP', 'PRCL'}}\n",
      "с\t613\t{'count': 613, 'tagset_set': {'PREP', 'PRCL'}}\n",
      "и\t574\t{'count': 574, 'tagset_set': {'CONJ', 'PRCL'}}\n",
      "о\t213\t{'count': 213, 'tagset_set': {'PREP', 'INTJ'}}\n",
      "году\t115\t{'count': 115, 'tagset_set': {'NOUN;inan;masc;sing;loc2', 'NOUN;inan;masc;sing;datv'}}\n",
      "а\t113\t{'count': 113, 'tagset_set': {'CONJ', 'INTJ'}}\n",
      "этом\t104\t{'count': 104, 'tagset_set': {'ADJF;Subx;Apro;Anph;masc;sing;loct', 'NPRO;neut;sing;loct', 'ADJF;Subx;Apro;Anph;neut;sing;loct'}}\n",
      "россии\t91\t{'count': 91, 'tagset_set': {'NOUN;inan;femn;Sgtm;Geox;sing;gent', 'NOUN;inan;femn;Sgtm;Geox;sing;datv', 'NOUN;inan;femn;Sgtm;Geox;sing;loct'}}\n",
      "было\t89\t{'count': 89, 'tagset_set': {'VERB;impf;intr;neut;sing;past;indc', 'PRCL'}}\n",
      "\n",
      "Cловоформы с самым большим количеством вариантов грамматических разборов:\n",
      "слово\tколичество тегсетов\tстатья\n",
      "сша\t6\t{'count': 44, 'tagset_set': {'NOUN;inan;GNdr;Pltm;Fixd;Abbr;Geox;plur;accs', 'NOUN;inan;GNdr;Pltm;Fixd;Abbr;Geox;plur;loct', 'NOUN;inan;GNdr;Pltm;Fixd;Abbr;Geox;plur;datv', 'NOUN;inan;GNdr;Pltm;Fixd;Abbr;Geox;plur;ablt', 'NOUN;inan;GNdr;Pltm;Fixd;Abbr;Geox;plur;gent', 'NOUN;inan;GNdr;Pltm;Fixd;Abbr;Geox;plur;nomn'}}\n",
      "кино\t5\t{'count': 15, 'tagset_set': {'NOUN;inan;neut;Fixd;sing;nomn', 'NOUN;inan;neut;Fixd;sing;loct', 'NOUN;inan;neut;Fixd;sing;gent', 'NOUN;inan;neut;Fixd;sing;accs', 'NOUN;inan;neut;Fixd;sing;datv'}}\n",
      "евро\t5\t{'count': 15, 'tagset_set': {'NOUN;inan;neut;Fixd;plur;gent', 'NOUN;inan;neut;Fixd;sing;nomn', 'NOUN;inan;neut;Fixd;sing;gent', 'NOUN;inan;neut;Fixd;sing;accs', 'NOUN;inan;neut;Fixd;sing;datv'}}\n",
      "компании\t5\t{'count': 24, 'tagset_set': {'NOUN;inan;femn;sing;datv', 'NOUN;inan;femn;sing;loct', 'NOUN;inan;femn;plur;accs', 'NOUN;inan;femn;sing;gent', 'NOUN;inan;femn;plur;nomn'}}\n",
      "пути\t5\t{'count': 11, 'tagset_set': {'NOUN;inan;masc;sing;datv', 'NOUN;inan;masc;plur;nomn', 'NOUN;inan;masc;plur;accs', 'NOUN;inan;masc;sing;gent', 'NOUN;inan;masc;sing;loct'}}\n",
      "какой\t5\t{'count': 7, 'tagset_set': {'ADJF;Apro;masc;sing;nomn', 'ADJF;Apro;femn;sing;loct', 'ADJF;Apro;femn;sing;gent', 'ADJF;Apro;femn;sing;ablt', 'ADJF;Apro;femn;sing;datv'}}\n",
      "это\t4\t{'count': 6, 'tagset_set': {'NPRO;neut;sing;accs', 'NPRO;neut;sing;nomn', 'ADJF;Subx;Apro;Anph;neut;sing;accs', 'PRCL'}}\n",
      "одной\t4\t{'count': 22, 'tagset_set': {'ADJF;Apro;Anum;femn;sing;ablt', 'ADJF;Apro;Anum;femn;sing;datv', 'ADJF;Apro;Anum;femn;sing;gent', 'ADJF;Apro;Anum;femn;sing;loct'}}\n",
      "своей\t4\t{'count': 21, 'tagset_set': {'ADJF;Apro;Anph;femn;sing;datv', 'ADJF;Apro;Anph;femn;sing;gent', 'ADJF;Apro;Anph;femn;sing;loct', 'ADJF;Apro;Anph;femn;sing;ablt'}}\n",
      "лица\t4\t{'count': 10, 'tagset_set': {'NOUN;anim;neut;sing;gent', 'NOUN;inan;neut;sing;gent', 'NOUN;anim;neut;plur;nomn', 'NOUN;inan;neut;plur;accs'}}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter_manytagsets = Counter({\n",
    "    key: len(d_ambig[key][\"tagset_set\"])\n",
    "    for key\n",
    "    in d_ambig\n",
    "})\n",
    "\n",
    "\n",
    "counter_manyforms   = Counter({\n",
    "    key: d_ambig[key][\"count\"]\n",
    "    for key\n",
    "    in d_ambig\n",
    "})\n",
    "\n",
    "print(\"Cамые частотные морфологически омонимичные словоформы:\")\n",
    "print(\"слово\", \"количество форм\", \"статья\", sep=\"\\t\")\n",
    "for word, count in counter_manyforms.most_common(10):\n",
    "    print(word, count, d[word], sep=\"\\t\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Cловоформы с самым большим количеством вариантов грамматических разборов:\")\n",
    "print(\"слово\", \"количество тегсетов\", \"статья\", sep=\"\\t\")\n",
    "for word, count in counter_manytagsets.most_common(10):\n",
    "    print(word, count, d[word], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b29c9c",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "Загрузите один и з файлов корпуса Syntagrus - https://github.com/UniversalDependencies/UD_Russian-SynTagRus/tree/master (можно взять тестовый)\n",
    "\n",
    "Преобразуйте все разборы предложений в графовые структуры через DependencyGraph, выберите 3 любых отношения и для каждого найдите топ-5 самых встречаемых пар слов, связанных этим отношением. \n",
    "\n",
    "Для самой частотной пары слов в каждом из отношений вытащите все подзависимые слова для каждого из них во всех предложениях (используя `flatten(get_subtree(d.nodes, index_of_a_word)` и сортируя результат по порядку слов в предложениях, аналогично тому как я делал с summaries только у вас будет два слова) \n",
    "В итоге у вас должен получится что-то такое:\n",
    "\n",
    "```\n",
    "### отношение\n",
    "relation_name\n",
    "\n",
    "### топ 5 пар слов связанных этим отношением\n",
    "(word1, word2), (word3, word4), (word5, word6), (word7, word8), (word9, word10)\n",
    "\n",
    "### подзависимые для самого частотного\n",
    "(subword word1 subword, word2 subword subword)\n",
    "\n",
    "... (и так три раза)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8cc3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data/ru_syntagrus-ud-test.conllu\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = [sentence.split(\"\\n\") for sentence in file.read().split('\\n\\n')]\n",
    "    data = tuple(map(\n",
    "        lambda sentence: \"\\n\".join([tok for tok in sentence if not tok.startswith(\"#\")]),\n",
    "        data\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e557d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    flat = []\n",
    "    for el in l:\n",
    "        if not isinstance(el, list):\n",
    "            flat.append(el)\n",
    "        else:\n",
    "            flat += flatten(el)\n",
    "    return flat\n",
    "\n",
    "def get_subtree(nodes, node):\n",
    "    \n",
    "    \n",
    "    if not nodes[node]['deps']:\n",
    "        return [node]\n",
    "    \n",
    "    else:\n",
    "        return [node] + [get_subtree(nodes, dep) for rel in nodes[node]['deps'] \n",
    "                         if rel != 'punct'  # пунктуацию доставать не будем\n",
    "                         for dep in nodes[node]['deps'][rel]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2345e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38967dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for sentence in data:\n",
    "    d = DependencyGraph(sentence, cell_separator=\"\\t\")\n",
    "    corpus.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee8144",
   "metadata": {},
   "source": [
    "###  Отношение NOUN + amod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a01a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noun_to_amod_forms : dict[str, dict[str, int]] = dict()\n",
    "# noun_to_amod_lemmas: dict[str, dict[str, int]] = dict()\n",
    "\n",
    "\n",
    "for d in corpus:\n",
    "    for node_i, node in d.nodes.items():\n",
    "        if node['ctag'] == 'NOUN' and 'amod' in node['deps']:\n",
    "\n",
    "            noun = node\n",
    "            attrs = [d.nodes[i] for i in node['deps']['amod']]\n",
    "\n",
    "            for attr in attrs:\n",
    "                phrase_form  = f'{attr[\"word\"]} {noun[\"word\"]}'\n",
    "                noun_to_amod_forms[phrase_form] = noun_to_amod_forms.get(phrase_form, 0) + 1\n",
    "\n",
    "                # phrase_lemma = f'{attr[\"lemma\"]} {noun[\"lemma\"]}'\n",
    "                # noun_to_amod_lemmas[phrase_lemma] = noun_to_amod_lemmas.get(phrase_lemma, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0a65e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('самом деле', 18),\n",
       " ('последнее время', 14),\n",
       " ('другой стороны', 14),\n",
       " ('1933 года', 13),\n",
       " ('пенсионного возраста', 12)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(noun_to_amod_forms)\n",
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bef037a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amod, noun = counter.most_common(1)[0][0].split(\" \")\n",
    "flatten_trees = list()\n",
    "\n",
    "\n",
    "for d in corpus:\n",
    "    for node_i, node in d.nodes.items():\n",
    "        ##  Идея в том, что поддерево amod -- это поддерево noun\n",
    "        # if node['word'] in (amod, noun):\n",
    "        if node['word'] == noun:\n",
    "            flatten_trees.append([d.nodes[i][\"word\"] for i in sorted(flatten(get_subtree(d.nodes, node_i)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5c269e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в\n",
      "Измаильском\n",
      "деле\n",
      "Повсюду\n",
      "был\n",
      "он\n",
      "Платов\n",
      "присутственен\n",
      "и\n",
      "подавал\n",
      "пример\n",
      "храбрости\n",
      "На\n",
      "самом\n",
      "деле\n"
     ]
    }
   ],
   "source": [
    "for c in flatten_trees[0]:\n",
    "    print(c)\n",
    "for c in flatten_trees[1]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce8995c",
   "metadata": {},
   "source": [
    "###  Отношение VERB + advmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9523535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "verb_to_amod_forms : dict[str, dict[str, int]] = dict()\n",
    "\n",
    "for d in corpus:\n",
    "    for node_i, node in d.nodes.items():\n",
    "        if node['ctag'] == 'VERB' and 'advmod' in node['deps']:\n",
    "\n",
    "            verb = node\n",
    "            attrs = [d.nodes[i] for i in node['deps']['advmod'] if d.nodes[i]['ctag'] in ('ADV', \"ADJ\")]\n",
    "\n",
    "            for attr in attrs:\n",
    "                phrase_form  = f'{attr[\"lemma\"]} {verb[\"lemma\"]}'\n",
    "                verb_to_amod_forms[phrase_form] = verb_to_amod_forms.get(phrase_form, 0) + 1\n",
    "\n",
    "\n",
    "counter = Counter(verb_to_amod_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bc3c1f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('так называть', 32),\n",
       " ('так делать', 8),\n",
       " ('там быть', 8),\n",
       " ('быстро расти', 8),\n",
       " ('здесь быть', 6)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eef59fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "advmod, verb = counter.most_common(1)[0][0].split(\" \")\n",
    "flatten_trees_verb = list()\n",
    "flatten_trees_advmod = list()\n",
    "\n",
    "\n",
    "for d in corpus:\n",
    "    for node_i, node in d.nodes.items():\n",
    "\n",
    "        if node['lemma'] == verb:\n",
    "            flatten_trees_verb.append([d.nodes[i][\"word\"] for i in sorted(flatten(get_subtree(d.nodes, node_i)))])\n",
    "        elif node['lemma'] == advmod:\n",
    "            flatten_trees_advmod.append([d.nodes[i][\"word\"] for i in sorted(flatten(get_subtree(d.nodes, node_i)))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a3578637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['имен', 'своих', 'не', 'называть']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_trees_verb[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "73a85b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['не', 'так', 'как', 'из', 'зрительного', 'зала']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_trees_advmod[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab578a",
   "metadata": {},
   "source": [
    "### Отношение ROOT + any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d66a38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_to_next_forms : dict[str, dict[str, int]] = dict()\n",
    "\n",
    "for d in corpus:\n",
    "    for node_i, node in d.nodes.items():\n",
    "        if node['ctag'] == 'TOP':\n",
    "\n",
    "            root = node\n",
    "            attrs = [d.nodes[i] for i in node['deps'][\"root\"]]\n",
    "\n",
    "            for attr in attrs:\n",
    "                phrase_form  = f'{attr[\"lemma\"]}'\n",
    "                root_to_next_forms[phrase_form] = root_to_next_forms.get(phrase_form, 0) + 1\n",
    "\n",
    "\n",
    "counter = Counter(root_to_next_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4f12368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('мочь', 197), ('можно', 127), ('быть', 126), ('стать', 103), ('должен', 90)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0a8c0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = counter.most_common(1)[0][0]\n",
    "flatten_trees = list()\n",
    "\n",
    "\n",
    "for d in corpus:\n",
    "    for node_i, node in d.nodes.items():\n",
    "        if node['lemma'] == root:\n",
    "            flatten_trees.append([d.nodes[i][\"word\"] for i in sorted(flatten(get_subtree(d.nodes, node_i)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "57790a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Он', 'не', 'мог', 'уйти', 'обратно', 'с', 'полным', 'подносом']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_trees[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2d09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
